{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Penalized_logistic_regression_correction.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Mi63bIwZRqIX"},"source":["# Penalized logistic regression"]},{"cell_type":"markdown","metadata":{"id":"9bv_hmEwRzeB"},"source":["In this notebook, you will discover how to use penalized linear regression Lasso (L1), Ridge (L2) and Elasticnet (L1 + L2).\n","\n","These penalties integrated to the cost function will help you train less complex models to avoid overfitting."]},{"cell_type":"markdown","metadata":{"id":"6165F8g5SpVO"},"source":["# Packages importation"]},{"cell_type":"code","metadata":{"id":"OGTNRbpNRgE1"},"source":["# Importation of the data for our classification example\n","from sklearn.datasets import load_breast_cancer\n","\n","# Importation of the function to standardize the data\n","from sklearn.preprocessing import StandardScaler\n","\n","# Importation of the train_test_split function which split randomly our data \n","# into a train and test set\n","from sklearn.model_selection import train_test_split\n","\n","# Importation of the logistic regression algorithm\n","from sklearn.linear_model import LogisticRegression\n","\n","# Importation of the performance metrics\n","from sklearn.metrics import accuracy_score, precision_recall_curve, f1_score, roc_auc_score, roc_curve, confusion_matrix\n","\n","# Importation of the maplotlib package to create graphics\n","import matplotlib.pyplot as plt\n","\n","# Importation of numpy to use of vectors, matrices, tensors.\n","import numpy as np "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pR2icedzSss9"},"source":["#Data Importation"]},{"cell_type":"code","metadata":{"id":"mR_YM7z6SuPf"},"source":["# Data frame for our classification\n","breast_cancer = load_breast_cancer()\n","X_classif = breast_cancer.data[:, ]\n","y_classif = breast_cancer.target"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"loBVRk-bSyOw"},"source":["Use the Sklearn function *train_test_split* to split your dataset into two random set.\n","\n","Use a random_state of 123 and use 10% of your dataset for the test set.\n","\n","Feel free to use the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."]},{"cell_type":"code","metadata":{"id":"TyeO9fCgS7tn"},"source":["# Use the function train_test_split to create your train and test set\n","X_train_classif, X_test_classif, y_train_classif, y_test_classif = train_test_split(X_classif, y_classif, \n","                                                                    test_size=0.10, \n","                                                                    random_state=123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZkMjV7hLWnQK"},"source":["# Step 1 : Data standardization\n","\n","For the use of a linear model it is essential to go through a step of normalization of the data.\n","\n","This step allows to make the model interpretable but also to facilitate the convergence of the model.\n","\n","Feel free to use the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html)."]},{"cell_type":"code","metadata":{"id":"bNhI9DkXWq_s"},"source":["# Initialize the StandardScaler function\n","scaler = StandardScaler()\n","\n","# Fit the StandardScaler on the trainig set\n","scaler.fit(X_train_classif)\n","\n","# Standardization of the training set\n","X_train_classif_norm = scaler.transform(X_train_classif)\n","\n","# Standardization of the validation set\n","X_test_classif_norm = scaler.transform(X_test_classif)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q-vUcTC5WtHS","executionInfo":{"status":"ok","timestamp":1636377981628,"user_tz":-60,"elapsed":38,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"81a56fd2-517e-43af-8608-0a5905a1647f"},"source":["print('Mean of the training set : '+str(X_train_classif_norm.mean(axis=0)))\n","print('Standard deviation of the training set : '+str(X_train_classif_norm.std(axis=0)))\n","\n","print('Mean of the testing set : '+str(X_test_classif_norm.mean(axis=0)))\n","print('Standard deviation of the testing set : '+str(X_test_classif_norm.std(axis=0)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mean of the training set : [-5.13911830e-16 -3.05745013e-16  8.53483950e-16  1.10003152e-15\n","  1.54650598e-15  7.12049777e-16 -3.42607887e-17  1.31622144e-16\n"," -4.95350289e-15  7.66292411e-15  1.53913340e-15  1.52915874e-15\n","  2.89265140e-16 -2.73435788e-16  1.46914815e-15  1.42854478e-15\n","  1.64798730e-17 -4.59701721e-16 -1.22119112e-15 -1.23165367e-16\n","  7.01695646e-16  3.83720833e-15 -9.50628465e-16  2.82326246e-16\n","  4.31078784e-15 -3.93619599e-16  2.29850861e-16  1.19966970e-16\n"," -4.12918397e-15  3.31635761e-15]\n","Standard deviation of the training set : [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","Mean of the testing set : [ 0.05124296 -0.18874973  0.03751984  0.03937833 -0.08482864 -0.18283252\n"," -0.07809671 -0.05164212 -0.19793932 -0.29018854  0.16691644 -0.05227523\n","  0.14396937  0.1740289   0.00614669 -0.25922462 -0.16597387 -0.12024983\n"," -0.11072004 -0.29499964  0.04435633 -0.07442118  0.0294974   0.01545994\n"," -0.02918828 -0.1089334  -0.05586924 -0.0412101  -0.06118219 -0.18438358]\n","Standard deviation of the testing set : [0.91480598 0.66909629 0.90978263 1.03432425 0.99367801 0.87525274\n"," 0.90633893 0.90440753 0.71909467 0.84083134 1.57807166 0.87352948\n"," 1.60204167 1.78225409 1.01625717 0.75721494 0.65688383 0.93341027\n"," 0.99874551 0.56807441 0.84475039 0.80330342 0.84141439 0.89898207\n"," 0.95859295 1.1139185  1.010088   0.95613581 1.03214445 1.24999311]\n"]}]},{"cell_type":"markdown","metadata":{"id":"kMM_d7MVXAop"},"source":["Answers expected :\n","\n","Mean of the training set : [-5.13911830e-16 -3.05745013e-16  8.53483950e-16  1.10003152e-15\n","  1.54650598e-15  7.12049777e-16 -3.42607887e-17  1.31622144e-16\n"," -4.95350289e-15  7.66292411e-15  1.53913340e-15  1.52915874e-15\n","  2.89265140e-16 -2.73435788e-16  1.46914815e-15  1.42854478e-15\n","  1.64798730e-17 -4.59701721e-16 -1.22119112e-15 -1.23165367e-16\n","  7.01695646e-16  3.83720833e-15 -9.50628465e-16  2.82326246e-16\n","  4.31078784e-15 -3.93619599e-16  2.29850861e-16  1.19966970e-16\n"," -4.12918397e-15  3.31635761e-15]\n","\n","\n","Standard deviation of the training set : [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n"," 1. 1. 1. 1. 1. 1.]\n","\n","\n","Mean of the testing set : [ 0.05124296 -0.18874973  0.03751984  0.03937833 -0.08482864 -0.18283252\n"," -0.07809671 -0.05164212 -0.19793932 -0.29018854  0.16691644 -0.05227523\n","  0.14396937  0.1740289   0.00614669 -0.25922462 -0.16597387 -0.12024983\n"," -0.11072004 -0.29499964  0.04435633 -0.07442118  0.0294974   0.01545994\n"," -0.02918828 -0.1089334  -0.05586924 -0.0412101  -0.06118219 -0.18438358]\n","\n"," \n","Standard deviation of the testing set : [0.91480598 0.66909629 0.90978263 1.03432425 0.99367801 0.87525274\n"," 0.90633893 0.90440753 0.71909467 0.84083134 1.57807166 0.87352948\n"," 1.60204167 1.78225409 1.01625717 0.75721494 0.65688383 0.93341027\n"," 0.99874551 0.56807441 0.84475039 0.80330342 0.84141439 0.89898207\n"," 0.95859295 1.1139185  1.010088   0.95613581 1.03214445 1.24999311]"]},{"cell_type":"markdown","metadata":{"id":"GO0eskyBXDHn"},"source":["# Step 2 : Model initialization"]},{"cell_type":"markdown","metadata":{"id":"SdmP_FNcXEuJ"},"source":["In the case of regression, there is no choice of hyperparameter.\n","\n","It is therefore sufficient to just initialize the function.\n","\n","Feel free to use the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."]},{"cell_type":"code","metadata":{"id":"LnHO1Y6-W_1B"},"source":["reg = LogisticRegression()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dYFJhEGdXNKw"},"source":["In the case of lasso regression, you have to choose a value for alpha.\n","\n","Alpha will control the regularization of the model.\n","\n","$ J(w) =  \\frac{1}{2m}[\\frac{1}{C}\\sum^m_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2+\\sum^n_{j=1}|w_j|]$ \n","\n","For logistic regression in Sklearn you will use the same function for all penalization and just precise the type of penalty you want with the parameter *penalty*.\n","\n","For this example initialize the regression with an alpha of 0.8, solver='saga' and a random_state of 123.\n","\n","Feel free to use the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."]},{"cell_type":"code","metadata":{"id":"Gd9OqjhcXO2P"},"source":["lasso = LogisticRegression(penalty='l1', C=0.8, random_state=123, solver='saga')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DdAWl4biXQWE"},"source":["In the case of Ridge regression, you have to choose a value for alpha.\n","\n","Alpha will control the regularization of the model.\n","\n","$ J(w) =  \\frac{1}{2m}[\\frac{1}{C}\\sum^m_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2+\\sum^n_{j=1}w_j^2]$ \n","\n","For logistic regression in Sklearn you will use the same function for all penalization and just precise the type of penalty you want with the parameter *penalty*.\n","\n","For this example initialize the regression with an alpha of 0.8 and a random_state of 123.\n","\n","Feel free to use the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."]},{"cell_type":"code","metadata":{"id":"GjL9ifDFXQ6e"},"source":["ridge = LogisticRegression(penalty='l2', C=0.8, random_state=123)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y50zJR2zXRcd"},"source":["In the case of elasticnet regression, you have to choose a value for alpha and ratio.\n","\n","Alpha will control the regularization of the model.\n","\n","ratio is the mixing parameter beween lasso (ratio=0) and ridge (ratio=1)\n","\n","$ J(w) =  \\frac{1}{2m}[\\frac{1}{C}\\sum^m_{i=1}(\\hat{y}^{(i)}-y^{(i)})^2+\\frac{1-ratio}{2}\\sum^n_{j=1}w_j^2 + ratio\\sum^n_{j=1}|w_j|]$ \n","\n","For logistic regression in Sklearn you will use the same function for all penalization and just precise the type of penalty you want with the parameter *penalty*.\n","\n","For this example initialize the regression with an alpha of 0.8, a ratio of 0.5, solver equal saga and a random_state of 123.\n","\n","Feel free to use the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)."]},{"cell_type":"code","metadata":{"id":"1UBh-J23XRr_"},"source":["elasticnet = LogisticRegression(penalty='elasticnet', C=0.8, l1_ratio=0.5, \n","                                random_state=123, solver='saga')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aa_W0KgA8ckm"},"source":["# Step 3 : Model training"]},{"cell_type":"markdown","metadata":{"id":"-A6JKhXgmPJx"},"source":["You must train the four models."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeabnIY1mYRx","executionInfo":{"status":"ok","timestamp":1636377981635,"user_tz":-60,"elapsed":25,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"bf287825-28a2-45fa-9d3c-154db0055224"},"source":["# Classic linear regression\n","reg.fit(X_train_classif_norm, y_train_classif)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XwaJMN79mZRc","executionInfo":{"status":"ok","timestamp":1636377982102,"user_tz":-60,"elapsed":488,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"972bc0c4-5c1a-4b6d-d8bb-e2134a269d4a"},"source":["# Lasso regression\n","lasso.fit(X_train_classif_norm, y_train_classif)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=0.8, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l1',\n","                   random_state=123, solver='saga', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KadiiuyRmZgT","executionInfo":{"status":"ok","timestamp":1636377982103,"user_tz":-60,"elapsed":35,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"5018f09e-b668-47c1-c1ab-b3367f0c97ed"},"source":["# Ridge regression\n","ridge.fit(X_train_classif_norm, y_train_classif)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=0.8, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-klrFThemZxG","executionInfo":{"status":"ok","timestamp":1636377982104,"user_tz":-60,"elapsed":30,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"12d083a3-155e-47e8-ae82-63320eb13216"},"source":["# ElasticNet regression\n","elasticnet.fit(X_train_classif_norm, y_train_classif)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"]},{"output_type":"execute_result","data":{"text/plain":["LogisticRegression(C=0.8, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=0.5, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='elasticnet',\n","                   random_state=123, solver='saga', tol=0.0001, verbose=0,\n","                   warm_start=False)"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","metadata":{"id":"NnfEUR7W8hoj"},"source":["# Step 4 : Model validation"]},{"cell_type":"markdown","metadata":{"id":"bEtaZWX7mrrY"},"source":["Your model is now trained, use it to predict the probability for your training and testing set for the four models."]},{"cell_type":"code","metadata":{"id":"SiojbymLmyaa"},"source":["# Classic linear regression one hot prediction\n","x_train_reg_prediction = reg.predict(X_train_classif_norm)\n","\n","x_test_reg_prediction = reg.predict(X_test_classif_norm)\n","\n","# Classic linear regression probability \n","x_train_reg_prediction_proba = reg.predict_proba(X_train_classif_norm)[:, 1]\n","\n","x_test_reg_prediction_proba = reg.predict_proba(X_test_classif_norm)[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MOkNCCvjmyTN"},"source":["# Lasso regression\n","x_train_lasso_prediction = lasso.predict(X_train_classif_norm)\n","\n","x_test_lasso_prediction = lasso.predict(X_test_classif_norm)\n","\n","# Classic linear regression probability \n","x_train_lasso_prediction_proba = lasso.predict_proba(X_train_classif_norm)[:, 1]\n","\n","x_test_lasso_prediction_proba = lasso.predict_proba(X_test_classif_norm)[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WmBIISUImyMw"},"source":["# Ridge regression\n","x_train_ridge_prediction = ridge.predict(X_train_classif_norm)\n","\n","x_test_ridge_prediction = ridge.predict(X_test_classif_norm)\n","\n","# Classic linear regression probability \n","x_train_ridge_prediction_proba = ridge.predict_proba(X_train_classif_norm)[:, 1]\n","\n","x_test_ridge_prediction_proba = ridge.predict_proba(X_test_classif_norm)[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EwNW7i3LmyFy"},"source":["# ElasticNet regression\n","x_train_elasticnet_prediction = elasticnet.predict(X_train_classif_norm)\n","\n","x_test_elasticnet_prediction = elasticnet.predict(X_test_classif_norm)\n","\n","# Classic linear regression probability \n","x_train_elasticnet_prediction_proba = elasticnet.predict_proba(X_train_classif_norm)[:, 1]\n","\n","x_test_elasticnet_prediction_proba = elasticnet.predict_proba(X_test_classif_norm)[:, 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nhMntqu3nLXg"},"source":["Compute the AUC for each model"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3laSbjqQnRuU","executionInfo":{"status":"ok","timestamp":1636377982108,"user_tz":-60,"elapsed":29,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"4f1b177a-d2a3-4fa8-bd5e-138dba4621aa"},"source":["# Classic linear regression\n","auc_train = roc_auc_score(y_train_classif, x_train_reg_prediction_proba)\n","\n","auc_test = roc_auc_score(y_test_classif, x_test_reg_prediction_proba)\n","\n","print('AUC for the training set : '+str(auc_train))\n","\n","print('AUC for the testing set : '+str(auc_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC for the training set : 0.9989821381665354\n","AUC for the testing set : 0.9797979797979799\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8XUQ18VonRoP","executionInfo":{"status":"ok","timestamp":1636377982108,"user_tz":-60,"elapsed":22,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"64c3b757-4b8c-466e-9681-60d1e2d9a157"},"source":["# Lasso regression\n","auc_train = roc_auc_score(y_train_classif, x_train_lasso_prediction_proba)\n","\n","auc_test = roc_auc_score(y_test_classif, x_test_lasso_prediction_proba)\n","\n","print('AUC for the training set : '+str(auc_train))\n","\n","print('AUC for the testing set : '+str(auc_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC for the training set : 0.9984732072498029\n","AUC for the testing set : 0.9810606060606061\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMNYDhdLnRiE","executionInfo":{"status":"ok","timestamp":1636377982109,"user_tz":-60,"elapsed":20,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"45b0e60f-ab39-4a47-df1c-88d7f1b23fec"},"source":["# Ridge regression\n","auc_train = roc_auc_score(y_train_classif, x_train_ridge_prediction_proba)\n","\n","auc_test = roc_auc_score(y_test_classif, x_test_ridge_prediction_proba)\n","\n","print('AUC for the training set : '+str(auc_train))\n","\n","print('AUC for the testing set : '+str(auc_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC for the training set : 0.9989493039138428\n","AUC for the testing set : 0.9797979797979799\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KR50bGBBnRcW","executionInfo":{"status":"ok","timestamp":1636377982109,"user_tz":-60,"elapsed":18,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"86c7c829-364c-4a26-88b6-e97ffb9b6f1e"},"source":["# ElasticNet regression\n","auc_train = roc_auc_score(y_train_classif, x_train_elasticnet_prediction_proba)\n","\n","auc_test = roc_auc_score(y_test_classif, x_test_elasticnet_prediction_proba)\n","\n","print('AUC for the training set : '+str(auc_train))\n","\n","print('AUC for the testing set : '+str(auc_test))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["AUC for the training set : 0.9985388757551878\n","AUC for the testing set : 0.9797979797979799\n"]}]},{"cell_type":"markdown","metadata":{"id":"-JZ4LWuO8p0c"},"source":["# Step 5 : Impact of the regularization term on the coefficient"]},{"cell_type":"markdown","metadata":{"id":"z3t6KupmuXAJ"},"source":["Impact of the regularization term for the Lasso regression."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jD8C0WgXpSFQ","executionInfo":{"status":"ok","timestamp":1636378258210,"user_tz":-60,"elapsed":232,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"9b618c95-c671-4f9d-9daa-8ec94c0976b7"},"source":["for alpha_values in [0.01, 0.1, 0.2, 0.5, 1] :\n","  lasso = LogisticRegression(penalty='l1', C=alpha_values, \n","                                random_state=123, solver='saga')\n","  lasso.fit(X_train_classif_norm, y_train_classif)\n","  print('Alpha = '+str(alpha_values))\n","  print(lasso.coef_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha = 0.01\n","[[ 0.          0.          0.          0.          0.          0.\n","   0.          0.          0.          0.          0.          0.\n","   0.          0.          0.          0.          0.          0.\n","   0.          0.         -0.26424952  0.         -0.16705349  0.\n","   0.          0.          0.         -0.53127883  0.          0.        ]]\n","Alpha = 0.1\n","[[-0.14507022  0.         -0.12041656  0.          0.          0.\n","   0.         -0.51510292  0.          0.         -0.24093316  0.\n","   0.          0.          0.          0.          0.          0.\n","   0.          0.         -0.91869054 -0.67675036 -0.70933674 -0.38701145\n","  -0.26333977  0.         -0.09230818 -0.83571792 -0.18593369  0.        ]]\n","Alpha = 0.2\n","[[-0.32047349 -0.1616604  -0.26753238 -0.2378532   0.          0.\n","   0.         -0.4705726   0.          0.         -0.51161577  0.\n","   0.         -0.17075759  0.          0.          0.          0.\n","   0.          0.02929312 -0.9082276  -0.79872879 -0.71663174 -0.61963469\n","  -0.55769254  0.         -0.25854068 -0.78820223 -0.32990701  0.        ]]\n","Alpha = 0.5\n","[[-0.44444855 -0.40696847 -0.40738401 -0.43583663  0.          0.\n","  -0.35461078 -0.58429947  0.          0.12569606 -0.76592696  0.00801643\n","  -0.31523647 -0.56021762  0.          0.28871981  0.          0.\n","   0.12038324  0.241992   -0.89618699 -0.92048883 -0.73840128 -0.76301502\n","  -0.84437162  0.         -0.54846451 -0.8332136  -0.5696944   0.        ]]\n","Alpha = 1\n","[[-0.49956233 -0.52281989 -0.47361553 -0.51902645  0.          0.\n","  -0.6015459  -0.70249323  0.          0.25768842 -0.91244215  0.17541411\n","  -0.50873419 -0.73945657 -0.14828455  0.51037357  0.          0.\n","   0.26310448  0.41094537 -0.93100073 -1.09753053 -0.78600855 -0.85133288\n","  -0.94370559  0.         -0.7493958  -0.91069814 -0.77234385  0.        ]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"]}]},{"cell_type":"markdown","metadata":{"id":"4xBwTZIbuzPB"},"source":["Impact of the regularization term for the Ridge regression."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C-QcyLRQu34C","executionInfo":{"status":"ok","timestamp":1636378268817,"user_tz":-60,"elapsed":275,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"7e5d694f-963a-468a-8d2a-295ef99e3a7c"},"source":["for alpha_values in [0.01, 0.1, 1, 10] :\n","  ridge = LogisticRegression(penalty='l2', C=alpha_values, \n","                                random_state=123, solver='saga')\n","  ridge.fit(X_train_classif_norm, y_train_classif)\n","  print('Alpha = '+str(alpha_values))\n","  print(ridge.coef_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha = 0.01\n","[[-0.22332807 -0.18183998 -0.22109315 -0.21005999 -0.08444644 -0.09918108\n","  -0.16677098 -0.21736004 -0.06543112  0.08783398 -0.16256353  0.02234146\n","  -0.13821362 -0.15414772  0.01046901  0.02683516  0.02003664 -0.05175978\n","   0.04688977  0.0771214  -0.24694482 -0.22193547 -0.23774733 -0.21891494\n","  -0.1751445  -0.1403803  -0.18043599 -0.24268607 -0.16691238 -0.07579839]]\n","Alpha = 0.1\n","[[-0.40740949 -0.41973909 -0.39714213 -0.39459714 -0.12862023 -0.02642965\n","  -0.374073   -0.43650351 -0.07770733  0.24929369 -0.44283409  0.0979864\n","  -0.31796138 -0.38646379 -0.09230164  0.24820669  0.03532078 -0.07610843\n","   0.16401597  0.2510048  -0.51996549 -0.59656969 -0.47592709 -0.46861481\n","  -0.44791095 -0.15665607 -0.41588697 -0.4977698  -0.43455213 -0.14280643]]\n","Alpha = 1\n","[[-0.56050357 -0.61516366 -0.54299684 -0.58942542 -0.14051486  0.17581356\n","  -0.72922041 -0.73140609 -0.06449194  0.43366429 -0.91011559  0.30619026\n","  -0.60449697 -0.79908697 -0.29597873  0.66939875 -0.01959067 -0.10291682\n","   0.36127102  0.59986868 -0.89478149 -1.14127237 -0.78070502 -0.85613101\n","  -0.85323908 -0.08753569 -0.8184763  -0.84851447 -0.84370488 -0.24482282]]\n","Alpha = 10\n","[[-0.62419726 -0.6923918  -0.60373858 -0.66863584 -0.16090353  0.23403771\n","  -0.84615469 -0.83977654 -0.06352595  0.49932741 -1.06424624  0.37932992\n","  -0.70183562 -0.94143323 -0.35796398  0.81174676 -0.03005087 -0.10734321\n","   0.42867137  0.7329329  -1.03690435 -1.335209   -0.89928875 -1.00288288\n","  -1.00460423 -0.07647437 -0.95794289 -0.98121359 -0.98645645 -0.27746043]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"]}]},{"cell_type":"markdown","metadata":{"id":"FKas_381vTjr"},"source":["Impact of the regularization term and the l1_ratio for the elasticnet regression."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cjwrGLXhuH8B","executionInfo":{"status":"ok","timestamp":1636378310595,"user_tz":-60,"elapsed":256,"user":{"displayName":"Gautherot Morgan","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"07974205866322024288"}},"outputId":"99900bc7-3631-4d33-843e-7a40421ee1c5"},"source":["for alpha_values, ratio_values in zip([0.01, 0.01, 0.1, 0.5, 1, 1], [0, 1, 0.5, 0.5, 1, 0]) :\n","  elasticnet = LogisticRegression(penalty='elasticnet', C=alpha_values, l1_ratio=ratio_values, \n","                                random_state=123, solver='saga')\n","  elasticnet.fit(X_train_classif_norm, y_train_classif)\n","  print('Alpha = '+str(alpha_values))\n","  print('ratio_values = '+str(ratio_values))\n","  print(elasticnet.coef_)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Alpha = 0.01\n","ratio_values = 0\n","[[-0.22332807 -0.18183998 -0.22109315 -0.21005999 -0.08444644 -0.09918108\n","  -0.16677098 -0.21736004 -0.06543112  0.08783398 -0.16256353  0.02234146\n","  -0.13821362 -0.15414772  0.01046901  0.02683516  0.02003664 -0.05175978\n","   0.04688977  0.0771214  -0.24694482 -0.22193547 -0.23774733 -0.21891494\n","  -0.1751445  -0.1403803  -0.18043599 -0.24268607 -0.16691238 -0.07579839]]\n","Alpha = 0.01\n","ratio_values = 1\n","[[ 0.          0.          0.          0.          0.          0.\n","   0.          0.          0.          0.          0.          0.\n","   0.          0.          0.          0.          0.          0.\n","   0.          0.         -0.26424952  0.         -0.16705349  0.\n","   0.          0.          0.         -0.53127883  0.          0.        ]]\n","Alpha = 0.1\n","ratio_values = 0.5\n","[[-0.35273939 -0.24863102 -0.33070369 -0.29041355  0.          0.\n","  -0.10552724 -0.42305855  0.          0.         -0.33737042  0.\n","  -0.0894243  -0.18854359  0.          0.          0.          0.\n","   0.          0.07626549 -0.61630964 -0.5688378  -0.53301465 -0.45161405\n","  -0.4706329   0.         -0.28159144 -0.60600002 -0.32294222  0.        ]]\n","Alpha = 0.5\n","ratio_values = 0.5\n","[[-0.47015469 -0.48502174 -0.44855412 -0.47625106  0.          0.\n","  -0.53415911 -0.6353275   0.          0.23255182 -0.78423208  0.13857422\n","  -0.448386   -0.63366039 -0.11859366  0.41524951  0.          0.\n","   0.22320067  0.34225448 -0.81729677 -0.95199687 -0.69924996 -0.73819185\n","  -0.81963894  0.         -0.65601491 -0.81021444 -0.68447462 -0.02014517]]\n","Alpha = 1\n","ratio_values = 1\n","[[-0.49956233 -0.52281989 -0.47361553 -0.51902645  0.          0.\n","  -0.6015459  -0.70249323  0.          0.25768842 -0.91244215  0.17541411\n","  -0.50873419 -0.73945657 -0.14828455  0.51037357  0.          0.\n","   0.26310448  0.41094537 -0.93100073 -1.09753053 -0.78600855 -0.85133288\n","  -0.94370559  0.         -0.7493958  -0.91069814 -0.77234385  0.        ]]\n","Alpha = 1\n","ratio_values = 0\n","[[-0.56050357 -0.61516366 -0.54299684 -0.58942542 -0.14051486  0.17581356\n","  -0.72922041 -0.73140609 -0.06449194  0.43366429 -0.91011559  0.30619026\n","  -0.60449697 -0.79908697 -0.29597873  0.66939875 -0.01959067 -0.10291682\n","   0.36127102  0.59986868 -0.89478149 -1.14127237 -0.78070502 -0.85613101\n","  -0.85323908 -0.08753569 -0.8184763  -0.84851447 -0.84370488 -0.24482282]]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n","/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_sag.py:330: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n","  \"the coef_ did not converge\", ConvergenceWarning)\n"]}]},{"cell_type":"code","metadata":{"id":"eG8phJVmwXEx"},"source":[""],"execution_count":null,"outputs":[]}]}