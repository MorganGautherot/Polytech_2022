{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gradient_Boosting_Tree.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MorganGautherot/Machine_Learning_Courses/blob/master/Random_Forest_%26_boosting.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wkNR7h9GRVy"
      },
      "source": [
        "# Gradient boosting tree"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVeSS0Lv5DG4"
      },
      "source": [
        "Now it's time to use the most advanced tree based model, the gradient boosting tree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5oiWNOb5U_Q"
      },
      "source": [
        "### Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9asF7fJa5V8u"
      },
      "source": [
        "# Load the library with the iris dataset\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load scikit's gradient boosting tree classifier library\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# Load pandas\n",
        "import pandas as pd\n",
        "\n",
        "# Load numpy\n",
        "import numpy as np\n",
        "\n",
        "# Compute accurasy_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set random seed\n",
        "np.random.seed(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj7IHTnM5olz"
      },
      "source": [
        "### Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOMWJdZH5mT0"
      },
      "source": [
        "# Create an object called iris with the iris data\n",
        "iris = load_iris()\n",
        "\n",
        "# Create a dataframe with the four feature variables\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "\n",
        "# View the top 5 rows\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpVuhS2a5rSx"
      },
      "source": [
        "# Add a new column with the species names, this is what we are going to try to predict\n",
        "df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)\n",
        "\n",
        "# View the top 5 rows\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gPGVAFG5ttr"
      },
      "source": [
        "### Train Test Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-QCpGAi5w4M"
      },
      "source": [
        "Use `train_test_split` to split your data into train and test dataset. \n",
        "\n",
        "If you need help feel free to check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sow9BqWs5wnf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "### Your code start here ###\n",
        "\n",
        "# Use .iloc to select only X data\n",
        "X = \n",
        "\n",
        "# Use pd.factorize the target value into 0, 1, 2 classes.\n",
        "y = \n",
        "\n",
        "# split train test using skleanr test_size = 0.33, and random_state = 42\n",
        "X_train, X_test, y_train, y_test = \n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z4ijzpST54kS"
      },
      "source": [
        "# Show the number of observations for the test and training dataframes\n",
        "print('Number of observations in the training data:', len(X_train))\n",
        "print('Number of observations in the test data:',len(X_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gVqsEWf953W6"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCsj4if457q4"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "# Instantiate model with 1000 decision trees and random state = 42\n",
        "gbt = \n",
        "\n",
        "# Train the model on training data\n",
        "\n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58LDhdv35_Pq"
      },
      "source": [
        "### Prediction on train and test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tfola5nj6BQ2"
      },
      "source": [
        "### Your code start here ###\n",
        "\n",
        "# Use the forest's predict method on the test data\n",
        "y_predict_train = \n",
        "y_predict_test = \n",
        "\n",
        "### Your code end here ###"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IL4csElk6B5v"
      },
      "source": [
        "### Compute accuracy for Training and Testing prediction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O5ezSrL6GJg"
      },
      "source": [
        "Now it's time to compute `accuracy_Score`.\n",
        "\n",
        "Feel free to check the [doc](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) if you need more informations "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZHIGBmZ6EMe"
      },
      "source": [
        "### Your code start here ###\n",
        "accuracy_train = \n",
        "accuracy_test = \n",
        "### Your code end here ###\n",
        "\n",
        "print(accuracy_train)\n",
        "print(accuracy_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6W6o3Z9W6IVy"
      },
      "source": [
        "### Decision Boundary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLBDSenL6LQx"
      },
      "source": [
        "# Parameters\n",
        "n_classes = 3\n",
        "plot_colors = \"ryb\"\n",
        "plot_step = 0.02\n",
        "\n",
        "# Load data\n",
        "iris = load_iris()\n",
        "\n",
        "for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3],\n",
        "                                [1, 2], [1, 3], [2, 3]]):\n",
        "    # We only take the two corresponding features\n",
        "    X = iris.data[:, pair]\n",
        "    y = iris.target\n",
        "\n",
        "    # Train\n",
        "    gbt = GradientBoostingClassifier(n_estimators=1000, random_state=42).fit(X, y)\n",
        "\n",
        "    # Plot the decision boundary\n",
        "    plt.subplot(2, 3, pairidx + 1)\n",
        "\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                         np.arange(y_min, y_max, plot_step))\n",
        "    plt.tight_layout(h_pad=0.5, w_pad=0.5, pad=2.5)\n",
        "\n",
        "    Z = gbt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "    plt.xlabel(iris.feature_names[pair[0]])\n",
        "    plt.ylabel(iris.feature_names[pair[1]])\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i],\n",
        "                    cmap=plt.cm.RdYlBu, edgecolor='black', s=15)\n",
        "\n",
        "plt.suptitle(\"Decision surface of a random forest using paired features\")\n",
        "plt.legend(loc='lower right', borderpad=0, handletextpad=0)\n",
        "plt.axis(\"tight\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJ7hJkTlLw9F"
      },
      "source": [
        "### Gradient boosting in practice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elDk16-1HMR7"
      },
      "source": [
        "Most of the challenges in applying GBRT successfully in practice can be illustrated in the context of a simple curve fitting example. Below you can see a regression problem with one feature x and the corresponding response y. \n",
        "\n",
        "We draw 100 training data points by picking an x coordinate uniformly at random, evaluating the ground truth (sinoid function; light blue line) and then adding some random gaussian noise. In addition to the 100 training points (blue) we also draw 100 test data points (red) which we will use the evaluate our approximation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luGlF0BlGQ6Q"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def ground_truth(x):\n",
        "  \"\"\"Ground truth -- function to approximate\"\"\"\n",
        "  return x * np.sin(x) + np.sin(2 * x)\n",
        "\n",
        "def gen_data(n_samples=200):\n",
        "  \"\"\"generate training and testing data\"\"\"\n",
        "  np.random.seed(13)\n",
        "  x = np.random.uniform(0, 10, size=n_samples)\n",
        "  x.sort()\n",
        "  y = ground_truth(x) + 0.75 * np.random.normal(size=n_samples)\n",
        "  train_mask = np.random.randint(0, 2, size=n_samples).astype(np.bool)\n",
        "  x_train, y_train = x[train_mask, np.newaxis], y[train_mask]\n",
        "  x_test, y_test = x[~train_mask, np.newaxis], y[~train_mask]\n",
        "  return x_train, x_test, y_train, y_test\n",
        "\n",
        "X_train, X_test, y_train, y_test = gen_data(200)\n",
        "\n",
        "# plot ground truth\n",
        "x_plot = np.linspace(0, 10, 500)\n",
        "\n",
        "def plot_data(figsize=(8, 5)):\n",
        "  fig = plt.figure(figsize=figsize)\n",
        "  gt = plt.plot(x_plot, ground_truth(x_plot), alpha=0.4, label='ground truth')\n",
        "\n",
        "# plot training and testing data\n",
        "plt.scatter(X_train, y_train, s=10, alpha=0.4)\n",
        "plt.scatter(X_test, y_test, s=10, alpha=0.4, color='red')\n",
        "plt.xlim((0, 10))\n",
        "plt.ylabel('y')\n",
        "plt.xlabel('x')\n",
        "plt.plot(x_plot, ground_truth(x_plot), alpha=0.4, label='ground truth')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DMkLUlJbIxtM"
      },
      "source": [
        "If you fit an individual regression tree to the above data you get a piece-wise constant approximation. The deeper you grow the tree, the more constant segments you can accommodate and thus, the more variance you can capture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZNF79ztHDHP"
      },
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "plot_data()\n",
        "\n",
        "est = DecisionTreeRegressor(max_depth=1).fit(X_train, y_train)\n",
        "\n",
        "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
        "\n",
        "         label='RT max_depth=1', color='g', alpha=0.9, linewidth=2)\n",
        "est = DecisionTreeRegressor(max_depth=3).fit(X_train, y_train)\n",
        "plt.plot(x_plot, est.predict(x_plot[:, np.newaxis]),\n",
        "label='RT max_depth=3', color='g', alpha=0.7, linewidth=1)\n",
        "\n",
        "plt.legend(loc='upper left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pj8lQf_wIy0x"
      },
      "source": [
        "Now, let's fit a gradient boosting model to the training data and let's see how the approximation progresses as we add more and more trees. The scikit-learn gradient boosting estimators allow you to evaluate the prediction of a model as a function of the number of trees via the staged_(predict|predict_proba) methods. These return a generator that iterates over the predictions as you add more and more trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asljjEufIrab"
      },
      "source": [
        "from itertools import islice\n",
        "from  sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "plot_data()\n",
        "\n",
        "est = GradientBoostingRegressor(n_estimators=1000, max_depth=1, learning_rate=1.0)\n",
        "est.fit(X_train, y_train)\n",
        "\n",
        "ax = plt.gca()\n",
        "first = True\n",
        "\n",
        "# step over prediction as we added 20 more trees.\n",
        "for pred in islice(est.staged_predict(x_plot[:, np.newaxis]), 0, 1000, 10):\n",
        "  plt.plot(x_plot, pred, color='r', alpha=0.2)\n",
        "  if first:\n",
        "    ax.annotate('High bias - low variance', xy=(x_plot[x_plot.shape[0] // 2],\n",
        "    pred[x_plot.shape[0] // 2]),\n",
        "    xycoords='data',\n",
        "    xytext=(3, 4), textcoords='data',\n",
        "    arrowprops=dict(arrowstyle=\"->\",\n",
        "    connectionstyle=\"arc\"))\n",
        "    first = False\n",
        "\n",
        "pred = est.predict(x_plot[:, np.newaxis])\n",
        "plt.plot(x_plot, pred, color='r', label='GBRT max_depth=1')\n",
        "ax.annotate('Low bias - high variance', xy=(x_plot[x_plot.shape[0] // 2],\n",
        "        pred[x_plot.shape[0] // 2]),\n",
        "        xycoords='data', xytext=(6.25, -6),\n",
        "        textcoords='data', arrowprops=dict(arrowstyle=\"->\",\n",
        "        connectionstyle=\"arc\"))\n",
        "plt.legend(loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok4Uf_trKj1b"
      },
      "source": [
        "The above plot shows 50 red lines where each shows the response of the GBRT model after 20 trees have been added. It starts with a very crude approximation that can only fit more-or-less constant functions (i.e. High bias - low variance) but as we add more trees the more variance our model can capture resulting in the solid red line.\n",
        "\n",
        "We can see that the more trees we add to our GBRT model and the deeper the individual trees are the more variance we can capture thus the higher the complexity of our model. But as usual in machine learning model complexity comes at a price -- overfitting.\n",
        "\n",
        "An important diagnostic when using GBRT in practice is the so-called deviance plot that shows the training/testing error (or deviance) as a function of the number of trees."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TZHOiCDUJAAm"
      },
      "source": [
        "n_estimators = len(est.estimators_)\n",
        "def deviance_plot(est, X_test, y_test, ax=None, label='', train_color='#2c7bb6',\n",
        "  test_color='#d7191c', alpha=1.0):\n",
        "  \"\"\"Deviance plot for ``est``, use ``X_test`` and ``y_test`` for test error. \"\"\"\n",
        "  test_dev = np.empty(n_estimators)\n",
        "\n",
        "  for i, pred in enumerate(est.staged_predict(X_test)):\n",
        "    test_dev[i] = est.loss_(y_test, pred)\n",
        "\n",
        "  if ax is None:\n",
        "    fig = plt.figure(figsize=(8, 5))\n",
        "    ax = plt.gca()\n",
        "\n",
        "  ax.plot(np.arange(n_estimators) + 1, test_dev, color=test_color, label='Test %s' % label,\n",
        "  linewidth=2, alpha=alpha)\n",
        "  ax.plot(np.arange(n_estimators) + 1, est.train_score_, color=train_color,\n",
        "  label='Train %s' % label, linewidth=2, alpha=alpha)\n",
        "  ax.set_ylabel('Error')\n",
        "  ax.set_xlabel('n_estimators')\n",
        "  ax.set_ylim((0, 2))\n",
        "  \n",
        "  return test_dev, ax\n",
        "\n",
        "test_dev, ax = deviance_plot(est, X_test, y_test)\n",
        "ax.legend(loc='upper right')\n",
        "\n",
        "# add some annotations\n",
        "ax.annotate('Lowest test error', xy=(test_dev.argmin() + 1, test_dev.min() + 0.02), xycoords='data',\n",
        "xytext=(150, 1.0), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "\n",
        "ann = ax.annotate('', xy=(800, test_dev[799]), xycoords='data',\n",
        "xytext=(800, est.train_score_[799]), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"<->\"))\n",
        "ax.text(810, 0.25, 'train-test gap')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqrB520MLh5O"
      },
      "source": [
        "The blue line above shows the training error: it rapidly decreases in the beginning and then gradually slows down but keeps decreasing as we add more and more trees. The testing error (red line) too decreases rapidly in the beginning but then slows down and reaches its minimum fairly early (~50 trees) and then even starts increasing. This is what we call overfitting, at a certain point the model has so much capacity that it starts fitting the idiosyncrasies of the training data -- in our case the random gaussian noise component that we added -- and hence limiting its ability to generalize to new unseen data. A large gap between training and testing error is usually a sign of overfitting.\n",
        "\n",
        "The great thing about gradient boosting is that it provides a number of knobs to control overfitting. These are usually subsumed by the term regularization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY391K8ML1FT"
      },
      "source": [
        "### Regularization of gradient boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VTZPb-YUL86x"
      },
      "source": [
        "GBRT provide three knobs to control overfitting: tree structure, shrinkage, and randomization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gKMW2QBL-M3"
      },
      "source": [
        "The depth of the individual trees is one aspect of model complexity. The depth of the trees basically control the degree of feature interactions that your model can fit. For example, if you want to capture the interaction between a feature latitude and a feature longitude your trees need a depth of at least two to capture this. Unfortunately, the degree of feature interactions is not known in advance but it is usually fine to assume that it is fairly low -- in practice, a depth of 4-6 usually gives the best results. In scikit-learn you can constrain the depth of the trees using the max_depth argument.  \n",
        "\n",
        "Another way to control the depth of the trees is by enforcing a lower bound on the number of samples in a leaf: this will avoid unbalanced splits where a leaf is formed for just one extreme data point. In scikit-learn you can do this using the argument min_samples_leaf. This is effectively a means to introduce bias into your model with the hope to also reduce variance as shown in the example below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXje06qaMK9l"
      },
      "source": [
        "\n",
        "\n",
        "def fmt_params(params):\n",
        "  return \", \".join(\"{0}={1}\".format(key, val) for key, val in params.items())\n",
        "\n",
        "fig = plt.figure(figsize=(8, 5))\n",
        "ax = plt.gca()\n",
        "\n",
        "for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n",
        "({'min_samples_leaf': 3},\n",
        "('#fdae61', '#abd9e9'))]:\n",
        "  est = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1.0)\n",
        "  est.set_params(**params)\n",
        "  est.fit(X_train, y_train)\n",
        "\n",
        "  test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
        "  train_color=train_color, test_color=test_color)\n",
        "\n",
        "ax.annotate('Higher bias', xy=(900, est.train_score_[899]), xycoords='data',\n",
        "xytext=(600, 0.3), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "ax.annotate('Lower variance', xy=(900, test_dev[899]), xycoords='data',\n",
        "xytext=(600, 0.4), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wG2OBZWOpil"
      },
      "source": [
        "### Shrinkage"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLW43ifhOuYG"
      },
      "source": [
        "The most important regularization technique for GBRT is shrinkage: the idea is basically to do slow learning by shrinking the predictions of each individual tree by some small scalar, the learning_rate. By doing so the model has to re-enforce concepts. A lower learning_rate requires a higher number of n_estimators to get to the same level of training error -- so its trading runtime against accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpzLfjJ-MPoh"
      },
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n",
        "\n",
        "                                          ({'learning_rate': 0.1},\n",
        "\n",
        "                                           ('#fdae61', '#abd9e9'))]:\n",
        "\n",
        "    est = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1.0)\n",
        "\n",
        "    est.set_params(**params)\n",
        "\n",
        "    est.fit(X_train, y_train)\n",
        "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
        "    train_color=train_color, test_color=test_color)\n",
        "\n",
        "ax.annotate('Requires more trees', xy=(200, est.train_score_[199]), xycoords='data',\n",
        "xytext=(300, 1.0), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "ax.annotate('Lower test error', xy=(900, test_dev[899]), xycoords='data',\n",
        "xytext=(600, 0.5), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NnGP3ODPPH1"
      },
      "source": [
        "### Stochastic Gradient boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3VFliRyPXRo"
      },
      "source": [
        "Similar to RandomForest, introducing randomization into the tree building process can lead to higher accuracy. Scikit-learn provides two ways to introduce randomization: a) subsampling the training set before growing each tree (subsample) and b) subsampling the features before finding the best split node (max_features). Experience showed that the latter works better if there is a sufficient large number of features (>30). One thing worth noting is that both options reduce runtime.\n",
        "\n",
        "Below we show the effect of using subsample=0.5, i.e. growing each tree on 50% of the training data, on our toy example:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_6lPNqxPYrh"
      },
      "source": [
        "fig = plt.figure(figsize=(8, 5))\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "for params, (test_color, train_color) in [({}, ('#d7191c', '#2c7bb6')),\n",
        "\n",
        "                                          ({'learning_rate': 0.1, 'subsample': 0.5},\n",
        "\n",
        "                                           ('#fdae61', '#abd9e9'))]:\n",
        "\n",
        "    est = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1.0,\n",
        "\n",
        "                                    random_state=1)\n",
        "\n",
        "    est.set_params(**params)\n",
        "\n",
        "    est.fit(X_train, y_train)\n",
        "\n",
        "    test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params(params),\n",
        "\n",
        "                                 train_color=train_color, test_color=test_color)\n",
        "ax.annotate('Even lower test error', xy=(400, test_dev[399]), xycoords='data',\n",
        "xytext=(500, 0.5), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "\n",
        "est = GradientBoostingRegressor(n_estimators=n_estimators, max_depth=1, learning_rate=1.0,\n",
        "subsample=0.5)\n",
        "est.fit(X_train, y_train)\n",
        "test_dev, ax = deviance_plot(est, X_test, y_test, ax=ax, label=fmt_params({'subsample': 0.5}),\n",
        "train_color='#abd9e9', test_color='#fdae61', alpha=0.5)\n",
        "ax.annotate('Subsample alone does poorly', xy=(300, test_dev[299]), xycoords='data',\n",
        "xytext=(250, 1.0), textcoords='data',\n",
        "arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc\"),\n",
        ")\n",
        "plt.legend(loc='upper right', fontsize='small')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Uvj0whPoK-"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tQMvi9WePqty"
      },
      "source": [
        "We now have introduced a number of hyperparameters -- as usual in machine learning it is quite tedious to optimize them. Especially, since they interact with each other (learning_rate and n_estimators, learning_rate and subsample, max_depth and max_features). \n",
        "\n",
        "We usually follow this recipe to tune the hyperparameters for a gradient boosting model:\n",
        "\n",
        "Choose loss based on your problem at hand (ie. target metric)\n",
        "Pick n_estimators as large as (computationally) possible (e.g. 3000).\n",
        "Tune max_depth, learning_rate, min_samples_leaf, and max_features via grid search.\n",
        "Increase n_estimators even more and tune learning_rate again holding the other parameters fixed.\n",
        "Scikit-learn provides a convenient API for hyperparameter tuning and grid search:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tosuOnvaPtjk"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {'learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
        "'max_depth': [4, 6],\n",
        "'min_samples_leaf': [3, 5, 9, 17],\n",
        "# 'max_features': [1.0, 0.3, 0.1] ## not possible in our example (only 1 fx)\n",
        "}\n",
        "\n",
        "est = GradientBoostingRegressor(n_estimators=3000)\n",
        "# this may take some minutes\n",
        "gs_cv = GridSearchCV(est, param_grid, n_jobs=4).fit(X_train, y_train)\n",
        "\n",
        "# best hyperparameter setting\n",
        "gs_cv.best_params_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FEYVXbkI8Ucv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}