{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Régression_linéaire_avec_TensorFlow_correction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmh5qAX9HONe"
      },
      "source": [
        "# Régression linéaire avec TensorFlow \n",
        "\n",
        "Dans ce notebook, on va voir comment utiliser TensorFlow pour entraîner un modèle de régression linéaire sur le jeu de données de *Boston* de Sklearn. \n",
        "Le but sera de prédire  le prix des maisons en fonction de leurs caractéristiques. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiOzO53zH8rP"
      },
      "source": [
        "## Importation des packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVBa_RpuGyHU"
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.losses import MeanAbsoluteError\n",
        "from tensorflow.keras.optimizers import SGD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3mkfIMdaIBhj"
      },
      "source": [
        "## Importation des données"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KW2cb43H_3r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0dc27a3-b47b-462d-f6a9-4a0a3e68fb70"
      },
      "source": [
        "boston = load_boston()\n",
        "X = pd.DataFrame(data=boston['data'], columns=boston['feature_names'])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, boston['target'], test_size=0.33, random_state=42)\n",
        "\n",
        "std_scaler = StandardScaler().fit(X_train, y_train)\n",
        "\n",
        "X_train  =  std_scaler.transform(X_train)\n",
        "X_test = std_scaler.transform(X_test)\n",
        "\n",
        "X_train_tf = tf.convert_to_tensor(X_train)\n",
        "X_test_tf = tf.convert_to_tensor(X_test)\n",
        "y_train_tf = tf.convert_to_tensor(y_train)\n",
        "y_test_tf = tf.convert_to_tensor(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
            "\n",
            "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
            "    the documentation of this function for further details.\n",
            "\n",
            "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
            "    dataset unless the purpose of the code is to study and educate about\n",
            "    ethical issues in data science and machine learning.\n",
            "\n",
            "    In this special case, you can fetch the dataset from the original\n",
            "    source::\n",
            "\n",
            "        import pandas as pd\n",
            "        import numpy as np\n",
            "\n",
            "\n",
            "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
            "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
            "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
            "        target = raw_df.values[1::2, 2]\n",
            "\n",
            "    Alternative datasets include the California housing dataset (i.e.\n",
            "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
            "    dataset. You can load the datasets as follows::\n",
            "\n",
            "        from sklearn.datasets import fetch_california_housing\n",
            "        housing = fetch_california_housing()\n",
            "\n",
            "    for the California housing dataset and::\n",
            "\n",
            "        from sklearn.datasets import fetch_openml\n",
            "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
            "\n",
            "    for the Ames housing dataset.\n",
            "    \n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dszlQ3tGJ1ZD"
      },
      "source": [
        "## Création de l'architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPRWdvgoKMzY"
      },
      "source": [
        "La fonction `Dense` nous permet d'initialiser les poids de notre régression linéaire et d'appliquer une multiplication matricielle entre nos poids et nos exemples. \n",
        "\n",
        "Pour plus d'informatons, n'hésitez pas à lire la [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EBW5KYGZKDeZ"
      },
      "source": [
        "def linear_regression(input):\n",
        "  model = Sequential([Dense(input)])\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XH8SMd4JLuHJ"
      },
      "source": [
        "Initialiser notre modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Raa4Xv5-LwIg"
      },
      "source": [
        "rl_model = linear_regression(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovbgesSnPuJz"
      },
      "source": [
        "Prédire notre jeu d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XD7TNaSVPUCO"
      },
      "source": [
        "prediction = rl_model(X_train_tf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtnouH-NVwjC",
        "outputId": "d5d3635e-87ff-40d6-89dd-854e8caae03f"
      },
      "source": [
        "mean_absolute_error(prediction, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.970796443443405"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_XrDMfFJ5yg"
      },
      "source": [
        "## Définir la fonction de coût"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TICFolPYPzRU"
      },
      "source": [
        "Nous allons maintenant initialiser notre fonction de coût.\n",
        "\n",
        "Vous pouvez trouver les différentes fonctions de coût implémenter par TensorFlow dans la [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/losses)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IWDG65LIQWgw"
      },
      "source": [
        "loss = MeanAbsoluteError()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j12AccolJ73t"
      },
      "source": [
        "## Définir l'algorithme d'optimisation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3Y62ubGRIuZ"
      },
      "source": [
        "Nous allons maintenant initialiser notre fonction d'optimisation qui va mettre à jour nos poids. Il existe des améliorations du gradient descent censé être plus rapide et performant. \n",
        "\n",
        "Vous pouvez trouver les différents algorithmes d'optimisation implémenter par Tensorflow dans la [documentation](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiLwI4Z2R023",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "016e8273-a95d-4806-d437-f4ebde34f69a"
      },
      "source": [
        "opt = SGD(lr=0.01)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(SGD, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YA6hWkeEJ_ds"
      },
      "source": [
        "## Définir la fonction d'entraînement"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT8eCpUaSLxz"
      },
      "source": [
        "Nous allons maintenant initialiser la fonction d'entraînement de notre modèle en utilisant notre modèle, notre fonction de coût et notre algorithme d'optimisation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5iq3weNI2-l"
      },
      "source": [
        "def step(model, opt, loss, X, y):\n",
        "\n",
        "  # garder la trace de nos gradients\n",
        "  with tf.GradientTape() as tape:\n",
        "    # faire une prédiction en utilisant le modèle, puis calculer le coût\n",
        "    pred = model(X)\n",
        "    train_loss = loss(y, pred)\n",
        "    \n",
        "    # calculer les gradients en utilisant tape \n",
        "    grads = tape.gradient(train_loss, model.trainable_variables)\n",
        "\n",
        "  # mettre à jour les poids du modèle\n",
        "  opt.apply_gradients(zip(grads, model.trainable_variables))\n",
        " \n",
        "  return model, train_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP17TJdcTbAT"
      },
      "source": [
        "## Entraîner notre modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDLHMZXrTdGO"
      },
      "source": [
        "Il est maintenant temps d'entraîner notre modèle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9Fl1-rsTKw9",
        "outputId": "9ed99fdd-7db6-429c-e013-cc5f1d40f55f"
      },
      "source": [
        "epoch = 1000\n",
        "history_train = []\n",
        "history_test = []\n",
        "\n",
        "for e in range(epoch) : \n",
        "\n",
        "  # mise à jour des poids\n",
        "  rl_model, train_loss = step(rl_model, opt, loss, X_train_tf, y_train_tf)\n",
        "\n",
        "  # prédiction sur le jeu de test\n",
        "  test_pred = rl_model(X_test_tf)\n",
        "  test_loss = mean_absolute_error(test_pred, y_test)\n",
        "\n",
        "  # sauvegarde des coûts\n",
        "  history_train = np.append(history_train, train_loss)\n",
        "  history_test = np.append(history_test, test_loss)\n",
        "\n",
        "  print('train_loss : '+str(np.squeeze(train_loss))+ ' test_loss : '+str(test_loss))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train_loss : 22.970797 test_loss : 21.28858233343609\n",
            "train_loss : 22.960796 test_loss : 21.27858234966497\n",
            "train_loss : 22.950796 test_loss : 21.268582370154512\n",
            "train_loss : 22.940798 test_loss : 21.258582398964613\n",
            "train_loss : 22.930796 test_loss : 21.248582440668233\n",
            "train_loss : 22.920795 test_loss : 21.23858245631713\n",
            "train_loss : 22.910795 test_loss : 21.228582477721265\n",
            "train_loss : 22.900797 test_loss : 21.218582510591265\n",
            "train_loss : 22.890797 test_loss : 21.20858254805653\n",
            "train_loss : 22.880798 test_loss : 21.19858256312544\n",
            "train_loss : 22.870796 test_loss : 21.188582583101923\n",
            "train_loss : 22.860796 test_loss : 21.178582620745644\n",
            "train_loss : 22.850796 test_loss : 21.168582653303343\n",
            "train_loss : 22.840797 test_loss : 21.158582670915266\n",
            "train_loss : 22.830795 test_loss : 21.148582690356374\n",
            "train_loss : 22.820797 test_loss : 21.138582707477543\n",
            "train_loss : 22.810797 test_loss : 21.128582723617193\n",
            "train_loss : 22.800797 test_loss : 21.118582743950586\n",
            "train_loss : 22.790796 test_loss : 21.10858276125021\n",
            "train_loss : 22.780796 test_loss : 21.098582792291026\n",
            "train_loss : 22.770798 test_loss : 21.088582829220922\n",
            "train_loss : 22.760798 test_loss : 21.078582850089685\n",
            "train_loss : 22.750797 test_loss : 21.068582865872422\n",
            "train_loss : 22.740797 test_loss : 21.058582904631503\n",
            "train_loss : 22.730797 test_loss : 21.048582938259944\n",
            "train_loss : 22.720797 test_loss : 21.038582956719537\n",
            "train_loss : 22.710798 test_loss : 21.02858300301843\n",
            "train_loss : 22.700798 test_loss : 21.01858304253595\n",
            "train_loss : 22.690798 test_loss : 21.008583069918394\n",
            "train_loss : 22.680796 test_loss : 20.99858309676547\n",
            "train_loss : 22.670797 test_loss : 20.98858313539071\n",
            "train_loss : 22.660797 test_loss : 20.978583181689597\n",
            "train_loss : 22.650797 test_loss : 20.96858323120071\n",
            "train_loss : 22.640797 test_loss : 20.958583272502803\n",
            "train_loss : 22.630798 test_loss : 20.94858331844478\n",
            "train_loss : 22.620798 test_loss : 20.938583355463905\n",
            "train_loss : 22.610796 test_loss : 20.928583390341544\n",
            "train_loss : 22.600798 test_loss : 20.91858341353025\n",
            "train_loss : 22.590797 test_loss : 20.90858344965709\n",
            "train_loss : 22.5808 test_loss : 20.898583495599066\n",
            "train_loss : 22.570799 test_loss : 20.88858353377816\n",
            "train_loss : 22.560799 test_loss : 20.8785835889999\n",
            "train_loss : 22.550798 test_loss : 20.868583632354248\n",
            "train_loss : 22.540798 test_loss : 20.85858366955183\n",
            "train_loss : 22.530798 test_loss : 20.848583699254217\n",
            "train_loss : 22.520798 test_loss : 20.838583729402746\n",
            "train_loss : 22.510798 test_loss : 20.82858376285273\n",
            "train_loss : 22.500797 test_loss : 20.81858380772396\n",
            "train_loss : 22.490797 test_loss : 20.808583860536533\n",
            "train_loss : 22.480799 test_loss : 20.79858390674619\n",
            "train_loss : 22.470798 test_loss : 20.788583946799086\n",
            "train_loss : 22.460798 test_loss : 20.77858395758503\n",
            "train_loss : 22.450798 test_loss : 20.768583965694116\n",
            "train_loss : 22.440798 test_loss : 20.7585839763016\n",
            "train_loss : 22.430796 test_loss : 20.748583984053774\n",
            "train_loss : 22.4208 test_loss : 20.73858399466126\n",
            "train_loss : 22.410797 test_loss : 20.728584002770347\n",
            "train_loss : 22.400799 test_loss : 20.718584013734745\n",
            "train_loss : 22.390799 test_loss : 20.708584021843834\n",
            "train_loss : 22.380798 test_loss : 20.698584032094406\n",
            "train_loss : 22.370798 test_loss : 20.688584040560407\n",
            "train_loss : 22.360796 test_loss : 20.678584050810976\n",
            "train_loss : 22.3508 test_loss : 20.668584058920064\n",
            "train_loss : 22.340797 test_loss : 20.65858406952755\n",
            "train_loss : 22.3308 test_loss : 20.648584077636635\n",
            "train_loss : 22.320799 test_loss : 20.638584088601036\n",
            "train_loss : 22.310799 test_loss : 20.62858409671012\n",
            "train_loss : 22.300798 test_loss : 20.618584107496066\n",
            "train_loss : 22.290798 test_loss : 20.60858411560515\n",
            "train_loss : 22.2808 test_loss : 20.598584126569552\n",
            "train_loss : 22.270798 test_loss : 20.58858413450018\n",
            "train_loss : 22.260798 test_loss : 20.578584145821495\n",
            "train_loss : 22.250797 test_loss : 20.568584154287496\n",
            "train_loss : 22.240799 test_loss : 20.558584165251894\n",
            "train_loss : 22.230799 test_loss : 20.548584173360982\n",
            "train_loss : 22.220798 test_loss : 20.53858418432538\n",
            "train_loss : 22.2108 test_loss : 20.528584192612925\n",
            "train_loss : 22.200798 test_loss : 20.518584203577323\n",
            "train_loss : 22.190798 test_loss : 20.50858421168641\n",
            "train_loss : 22.180798 test_loss : 20.49858422282927\n",
            "train_loss : 22.1708 test_loss : 20.488584230938354\n",
            "train_loss : 22.160797 test_loss : 20.478584241902755\n",
            "train_loss : 22.150799 test_loss : 20.46858425001184\n",
            "train_loss : 22.140799 test_loss : 20.45858426097624\n",
            "train_loss : 22.130798 test_loss : 20.448584269085327\n",
            "train_loss : 22.120798 test_loss : 20.438584280049728\n",
            "train_loss : 22.110796 test_loss : 20.428584288158813\n",
            "train_loss : 22.1008 test_loss : 20.418584299123214\n",
            "train_loss : 22.090797 test_loss : 20.4085843072323\n",
            "train_loss : 22.0808 test_loss : 20.3985843181967\n",
            "train_loss : 22.070799 test_loss : 20.388584326662702\n",
            "train_loss : 22.060799 test_loss : 20.3785843376271\n",
            "train_loss : 22.050798 test_loss : 20.36858434573619\n",
            "train_loss : 22.040798 test_loss : 20.358584356700586\n",
            "train_loss : 22.0308 test_loss : 20.348584364809675\n",
            "train_loss : 22.020798 test_loss : 20.33858437541716\n",
            "train_loss : 22.010798 test_loss : 20.328584383526245\n",
            "train_loss : 22.000797 test_loss : 20.318584394312186\n",
            "train_loss : 21.990799 test_loss : 20.308584402064362\n",
            "train_loss : 21.980799 test_loss : 20.29858441302876\n",
            "train_loss : 21.970798 test_loss : 20.288584478957922\n",
            "train_loss : 21.9608 test_loss : 20.278584475645758\n",
            "train_loss : 21.950798 test_loss : 20.26858450088672\n",
            "train_loss : 21.940798 test_loss : 20.258584494719244\n",
            "train_loss : 21.930798 test_loss : 20.248584517104895\n",
            "train_loss : 21.9208 test_loss : 20.23858451379273\n",
            "train_loss : 21.910797 test_loss : 20.228584539033694\n",
            "train_loss : 21.900799 test_loss : 20.218584532866217\n",
            "train_loss : 21.890799 test_loss : 20.208584555251868\n",
            "train_loss : 21.880798 test_loss : 20.198584551939703\n",
            "train_loss : 21.870798 test_loss : 20.188584578251408\n",
            "train_loss : 21.860798 test_loss : 20.178584569228622\n",
            "train_loss : 21.8508 test_loss : 20.168584594469582\n",
            "train_loss : 21.8408 test_loss : 20.158584592585076\n",
            "train_loss : 21.8308 test_loss : 20.14858461639838\n",
            "train_loss : 21.820799 test_loss : 20.138584605947937\n",
            "train_loss : 21.810799 test_loss : 20.128584632973467\n",
            "train_loss : 21.800798 test_loss : 20.118584631445877\n",
            "train_loss : 21.790798 test_loss : 20.108584655259182\n",
            "train_loss : 21.7808 test_loss : 20.09858464480874\n",
            "train_loss : 21.770798 test_loss : 20.08858467183427\n",
            "train_loss : 21.760798 test_loss : 20.078584669949763\n",
            "train_loss : 21.7508 test_loss : 20.068584693763068\n",
            "train_loss : 21.740799 test_loss : 20.058584683312624\n",
            "train_loss : 21.730799 test_loss : 20.04858470998124\n",
            "train_loss : 21.7208 test_loss : 20.038584708096735\n",
            "train_loss : 21.7108 test_loss : 20.02858473191004\n",
            "train_loss : 21.700798 test_loss : 20.018584722887255\n",
            "train_loss : 21.6908 test_loss : 20.008584748128214\n",
            "train_loss : 21.680798 test_loss : 19.998584744816053\n",
            "train_loss : 21.6708 test_loss : 19.988584771484668\n",
            "train_loss : 21.660799 test_loss : 19.978584761034227\n",
            "train_loss : 21.650799 test_loss : 19.968584784847533\n",
            "train_loss : 21.640799 test_loss : 19.958584782963026\n",
            "train_loss : 21.630798 test_loss : 19.94858480963164\n",
            "train_loss : 21.620798 test_loss : 19.9385847991812\n",
            "train_loss : 21.610798 test_loss : 19.928584822994505\n",
            "train_loss : 21.6008 test_loss : 19.91858482111\n",
            "train_loss : 21.5908 test_loss : 19.908584847778613\n",
            "train_loss : 21.5808 test_loss : 19.898584838755827\n",
            "train_loss : 21.570799 test_loss : 19.888584861141478\n",
            "train_loss : 21.560799 test_loss : 19.87858485925697\n",
            "train_loss : 21.5508 test_loss : 19.868584885925586\n",
            "train_loss : 21.540798 test_loss : 19.85858487618897\n",
            "train_loss : 21.5308 test_loss : 19.84858489857462\n",
            "train_loss : 21.520798 test_loss : 19.83858489811777\n",
            "train_loss : 21.510798 test_loss : 19.82858492407256\n",
            "train_loss : 21.5008 test_loss : 19.818584913622118\n",
            "train_loss : 21.490799 test_loss : 19.808584937435423\n",
            "train_loss : 21.480799 test_loss : 19.798584935550917\n",
            "train_loss : 21.4708 test_loss : 19.78858496221953\n",
            "train_loss : 21.4608 test_loss : 19.77858495176909\n",
            "train_loss : 21.450798 test_loss : 19.768584975582396\n",
            "train_loss : 21.4408 test_loss : 19.75858497369789\n",
            "train_loss : 21.430798 test_loss : 19.748584999652678\n",
            "train_loss : 21.4208 test_loss : 19.738584989916063\n",
            "train_loss : 21.410799 test_loss : 19.728585016584677\n",
            "train_loss : 21.400799 test_loss : 19.718585011844862\n",
            "train_loss : 21.390799 test_loss : 19.70858503423051\n",
            "train_loss : 21.380798 test_loss : 19.698585027349207\n",
            "train_loss : 21.370798 test_loss : 19.688585056873137\n",
            "train_loss : 21.360798 test_loss : 19.678585048564177\n",
            "train_loss : 21.3508 test_loss : 19.668585072377482\n",
            "train_loss : 21.3408 test_loss : 19.65858506478235\n",
            "train_loss : 21.3308 test_loss : 19.648585097161593\n",
            "train_loss : 21.320799 test_loss : 19.63858508671115\n",
            "train_loss : 21.310799 test_loss : 19.628585107669142\n",
            "train_loss : 21.3008 test_loss : 19.618585102215494\n",
            "train_loss : 21.290798 test_loss : 19.60858513745005\n",
            "train_loss : 21.2808 test_loss : 19.598585124144293\n",
            "train_loss : 21.270798 test_loss : 19.588585145102286\n",
            "train_loss : 21.2608 test_loss : 19.578585140362467\n",
            "train_loss : 21.2508 test_loss : 19.568585174883193\n",
            "train_loss : 21.240799 test_loss : 19.55858516157744\n",
            "train_loss : 21.2308 test_loss : 19.548585182535433\n",
            "train_loss : 21.2208 test_loss : 19.538585179937098\n",
            "train_loss : 21.2108 test_loss : 19.528585213743995\n",
            "train_loss : 21.200798 test_loss : 19.518585199010584\n",
            "train_loss : 21.1908 test_loss : 19.50858522139623\n",
            "train_loss : 21.180798 test_loss : 19.49858521808407\n",
            "train_loss : 21.1708 test_loss : 19.488585251890967\n",
            "train_loss : 21.160799 test_loss : 19.478585236086815\n",
            "train_loss : 21.150799 test_loss : 19.46858525990012\n",
            "train_loss : 21.140799 test_loss : 19.458585258015614\n",
            "train_loss : 21.130798 test_loss : 19.44858529003794\n",
            "train_loss : 21.1208 test_loss : 19.43858527387687\n",
            "train_loss : 21.110798 test_loss : 19.428585297690177\n",
            "train_loss : 21.1008 test_loss : 19.41858529580567\n",
            "train_loss : 21.0908 test_loss : 19.408585328184913\n",
            "train_loss : 21.0808 test_loss : 19.398585312023844\n",
            "train_loss : 21.070799 test_loss : 19.388585335123324\n",
            "train_loss : 21.060799 test_loss : 19.378585333952643\n",
            "train_loss : 21.0508 test_loss : 19.368585367045714\n",
            "train_loss : 21.040798 test_loss : 19.35858534945699\n",
            "train_loss : 21.0308 test_loss : 19.34858537184264\n",
            "train_loss : 21.020798 test_loss : 19.33858537317036\n",
            "train_loss : 21.0108 test_loss : 19.328585407691087\n",
            "train_loss : 21.0008 test_loss : 19.318585385819393\n",
            "train_loss : 20.9908 test_loss : 19.308585410346524\n",
            "train_loss : 20.9808 test_loss : 19.298585414172642\n",
            "train_loss : 20.9708 test_loss : 19.28858544512423\n",
            "train_loss : 20.9608 test_loss : 19.27858542610785\n",
            "train_loss : 20.950798 test_loss : 19.268585449921154\n",
            "train_loss : 20.9408 test_loss : 19.25858545089196\n",
            "train_loss : 20.9308 test_loss : 19.248585483271203\n",
            "train_loss : 20.9208 test_loss : 19.23858546425482\n",
            "train_loss : 20.910799 test_loss : 19.228585488068127\n",
            "train_loss : 20.900799 test_loss : 19.218585489038933\n",
            "train_loss : 20.890799 test_loss : 19.208585521418176\n",
            "train_loss : 20.880798 test_loss : 19.198585502401794\n",
            "train_loss : 20.8708 test_loss : 19.18858552692893\n",
            "train_loss : 20.860798 test_loss : 19.178585527899735\n",
            "train_loss : 20.8508 test_loss : 19.168585560278977\n",
            "train_loss : 20.8408 test_loss : 19.158585541262596\n",
            "train_loss : 20.8308 test_loss : 19.1485855650759\n",
            "train_loss : 20.8208 test_loss : 19.13858556461905\n",
            "train_loss : 20.8108 test_loss : 19.12858559842595\n",
            "train_loss : 20.8008 test_loss : 19.11858558226488\n",
            "train_loss : 20.7908 test_loss : 19.108585603222874\n",
            "train_loss : 20.7808 test_loss : 19.09858559991071\n",
            "train_loss : 20.7708 test_loss : 19.088585635859094\n",
            "train_loss : 20.7608 test_loss : 19.078585621125683\n",
            "train_loss : 20.7508 test_loss : 19.068585640656018\n",
            "train_loss : 20.7408 test_loss : 19.058585637343857\n",
            "train_loss : 20.7308 test_loss : 19.048585673292237\n",
            "train_loss : 20.7208 test_loss : 19.03858565713117\n",
            "train_loss : 20.7108 test_loss : 19.02858567808916\n",
            "train_loss : 20.700798 test_loss : 19.018585676204655\n",
            "train_loss : 20.690802 test_loss : 19.00858571143921\n",
            "train_loss : 20.6808 test_loss : 18.99858569599197\n",
            "train_loss : 20.6708 test_loss : 18.988585715522305\n",
            "train_loss : 20.660799 test_loss : 18.97858571078249\n",
            "train_loss : 20.650799 test_loss : 18.968585748872353\n",
            "train_loss : 20.6408 test_loss : 18.9585857355666\n",
            "train_loss : 20.6308 test_loss : 18.948585753669278\n",
            "train_loss : 20.6208 test_loss : 18.938585750357117\n",
            "train_loss : 20.610802 test_loss : 18.928585787019326\n",
            "train_loss : 20.600801 test_loss : 18.918585771572086\n",
            "train_loss : 20.5908 test_loss : 18.90858579253008\n",
            "train_loss : 20.5808 test_loss : 18.89858578779026\n",
            "train_loss : 20.5708 test_loss : 18.888585825880128\n",
            "train_loss : 20.560799 test_loss : 18.878585811146717\n",
            "train_loss : 20.5508 test_loss : 18.868585829249398\n",
            "train_loss : 20.5408 test_loss : 18.85858582450958\n",
            "train_loss : 20.5308 test_loss : 18.84858586545476\n",
            "train_loss : 20.5208 test_loss : 18.83858584929369\n",
            "train_loss : 20.510801 test_loss : 18.82858586739637\n",
            "train_loss : 20.5008 test_loss : 18.818585861942722\n",
            "train_loss : 20.4908 test_loss : 18.808585901460244\n",
            "train_loss : 20.4808 test_loss : 18.798585886726833\n",
            "train_loss : 20.4708 test_loss : 18.78858590625717\n",
            "train_loss : 20.4608 test_loss : 18.778585901517353\n",
            "train_loss : 20.450798 test_loss : 18.768585939607217\n",
            "train_loss : 20.440802 test_loss : 18.75858592344615\n",
            "train_loss : 20.4308 test_loss : 18.748585942976487\n",
            "train_loss : 20.420801 test_loss : 18.738585939664326\n",
            "train_loss : 20.410799 test_loss : 18.728585979181847\n",
            "train_loss : 20.400799 test_loss : 18.718585961593124\n",
            "train_loss : 20.3908 test_loss : 18.70858598112346\n",
            "train_loss : 20.3808 test_loss : 18.698585977811298\n",
            "train_loss : 20.3708 test_loss : 18.68858601732882\n",
            "train_loss : 20.360802 test_loss : 18.678585999740097\n",
            "train_loss : 20.350801 test_loss : 18.668586020698086\n",
            "train_loss : 20.3408 test_loss : 18.65858601595827\n",
            "train_loss : 20.3308 test_loss : 18.648586054048135\n",
            "train_loss : 20.3208 test_loss : 18.63858603788707\n",
            "train_loss : 20.3108 test_loss : 18.62858605884506\n",
            "train_loss : 20.3008 test_loss : 18.618586054105243\n",
            "train_loss : 20.2908 test_loss : 18.608586093622765\n",
            "train_loss : 20.2808 test_loss : 18.598586076034042\n",
            "train_loss : 20.2708 test_loss : 18.588586095564377\n",
            "train_loss : 20.260801 test_loss : 18.578586092252216\n",
            "train_loss : 20.2508 test_loss : 18.568586131769738\n",
            "train_loss : 20.2408 test_loss : 18.558586114181015\n",
            "train_loss : 20.2308 test_loss : 18.54858613371135\n",
            "train_loss : 20.2208 test_loss : 18.53858613039919\n",
            "train_loss : 20.2108 test_loss : 18.52858616991671\n",
            "train_loss : 20.200798 test_loss : 18.518586153755642\n",
            "train_loss : 20.190802 test_loss : 18.508586171858322\n",
            "train_loss : 20.1808 test_loss : 18.498586167118503\n",
            "train_loss : 20.170801 test_loss : 18.488586208063683\n",
            "train_loss : 20.160799 test_loss : 18.478586191902615\n",
            "train_loss : 20.150799 test_loss : 18.468586210005295\n",
            "train_loss : 20.1408 test_loss : 18.458586205265476\n",
            "train_loss : 20.1308 test_loss : 18.448586244782998\n",
            "train_loss : 20.120802 test_loss : 18.438586230049587\n",
            "train_loss : 20.110802 test_loss : 18.428586249579922\n",
            "train_loss : 20.100801 test_loss : 18.41858624341245\n",
            "train_loss : 20.0908 test_loss : 18.40858628292997\n",
            "train_loss : 20.0808 test_loss : 18.39858626819656\n",
            "train_loss : 20.0708 test_loss : 18.388586287726895\n",
            "train_loss : 20.0608 test_loss : 18.378586280131767\n",
            "train_loss : 20.050802 test_loss : 18.368586321076943\n",
            "train_loss : 20.0408 test_loss : 18.35858630777119\n",
            "train_loss : 20.0308 test_loss : 18.348586325873868\n",
            "train_loss : 20.0208 test_loss : 18.33858631756491\n",
            "train_loss : 20.010801 test_loss : 18.328586359937745\n",
            "train_loss : 20.0008 test_loss : 18.318586348059647\n",
            "train_loss : 19.9908 test_loss : 18.30858636473467\n",
            "train_loss : 19.9808 test_loss : 18.298586354284225\n",
            "train_loss : 19.9708 test_loss : 18.288586398084718\n",
            "train_loss : 19.9608 test_loss : 18.27858638192365\n",
            "train_loss : 19.9508 test_loss : 18.268586402881642\n",
            "train_loss : 19.940802 test_loss : 18.258586389575886\n",
            "train_loss : 19.930801 test_loss : 18.24858643623169\n",
            "train_loss : 19.920801 test_loss : 18.23858642007062\n",
            "train_loss : 19.9108 test_loss : 18.228586441028614\n",
            "train_loss : 19.9008 test_loss : 18.218586427722858\n",
            "train_loss : 19.8908 test_loss : 18.208586473664834\n",
            "train_loss : 19.8808 test_loss : 18.198586458931423\n",
            "train_loss : 19.870802 test_loss : 18.188586479175587\n",
            "train_loss : 19.860802 test_loss : 18.17858646586983\n",
            "train_loss : 19.850801 test_loss : 18.168586512525636\n",
            "train_loss : 19.8408 test_loss : 18.158586496364567\n",
            "train_loss : 19.830801 test_loss : 18.14858651732256\n",
            "train_loss : 19.820803 test_loss : 18.138586504016804\n",
            "train_loss : 19.8108 test_loss : 18.12858655067261\n",
            "train_loss : 19.800802 test_loss : 18.11858653451154\n",
            "train_loss : 19.7908 test_loss : 18.108586555469532\n",
            "train_loss : 19.7808 test_loss : 18.098586542163776\n",
            "train_loss : 19.7708 test_loss : 18.08858658596427\n",
            "train_loss : 19.760801 test_loss : 18.07858657408617\n",
            "train_loss : 19.7508 test_loss : 18.068586594330334\n",
            "train_loss : 19.7408 test_loss : 18.058586581024578\n",
            "train_loss : 19.7308 test_loss : 18.04858662411124\n",
            "train_loss : 19.7208 test_loss : 18.038586612233143\n",
            "train_loss : 19.7108 test_loss : 18.028586631763478\n",
            "train_loss : 19.7008 test_loss : 18.01858661988538\n",
            "train_loss : 19.690804 test_loss : 18.008586662258214\n",
            "train_loss : 19.680801 test_loss : 17.998586650380116\n",
            "train_loss : 19.670801 test_loss : 17.98858666991045\n",
            "train_loss : 19.6608 test_loss : 17.978586658032352\n",
            "train_loss : 19.6508 test_loss : 17.968586698977532\n",
            "train_loss : 19.6408 test_loss : 17.958586688527088\n",
            "train_loss : 19.6308 test_loss : 17.94858670662977\n",
            "train_loss : 19.620802 test_loss : 17.938586696179325\n",
            "train_loss : 19.610802 test_loss : 17.928586737124505\n",
            "train_loss : 19.600801 test_loss : 17.918586725246403\n",
            "train_loss : 19.5908 test_loss : 17.90858674477674\n",
            "train_loss : 19.580801 test_loss : 17.898586731470985\n",
            "train_loss : 19.570803 test_loss : 17.88858677384382\n",
            "train_loss : 19.5608 test_loss : 17.878586764821033\n",
            "train_loss : 19.550802 test_loss : 17.868586781496056\n",
            "train_loss : 19.5408 test_loss : 17.858586769617958\n",
            "train_loss : 19.5308 test_loss : 17.848589884306854\n",
            "train_loss : 19.5208 test_loss : 17.83870963707655\n",
            "train_loss : 19.510801 test_loss : 17.828829413402577\n",
            "train_loss : 19.5008 test_loss : 17.818949154751028\n",
            "train_loss : 19.4908 test_loss : 17.80906896462697\n",
            "train_loss : 19.4808 test_loss : 17.79918871454136\n",
            "train_loss : 19.4708 test_loss : 17.789308493008868\n",
            "train_loss : 19.4608 test_loss : 17.779428235784973\n",
            "train_loss : 19.4508 test_loss : 17.769548043519436\n",
            "train_loss : 19.440804 test_loss : 17.759667799144445\n",
            "train_loss : 19.430801 test_loss : 17.74978757190133\n",
            "train_loss : 19.420801 test_loss : 17.739907318960405\n",
            "train_loss : 19.4108 test_loss : 17.730027123125726\n",
            "train_loss : 19.4008 test_loss : 17.720146878750736\n",
            "train_loss : 19.3908 test_loss : 17.710386409017136\n",
            "train_loss : 19.3808 test_loss : 17.70062591358573\n",
            "train_loss : 19.370802 test_loss : 17.69086548097119\n",
            "train_loss : 19.360802 test_loss : 17.681104992678062\n",
            "train_loss : 19.350801 test_loss : 17.671344530082745\n",
            "train_loss : 19.3408 test_loss : 17.66158403607899\n",
            "train_loss : 19.330801 test_loss : 17.6518236020368\n",
            "train_loss : 19.320803 test_loss : 17.64206311374367\n",
            "train_loss : 19.310802 test_loss : 17.632302651148354\n",
            "train_loss : 19.300802 test_loss : 17.62254215857226\n",
            "train_loss : 19.2908 test_loss : 17.612781723102408\n",
            "train_loss : 19.280802 test_loss : 17.603021236236934\n",
            "train_loss : 19.2708 test_loss : 17.593260772213963\n",
            "train_loss : 19.260801 test_loss : 17.583500280351693\n",
            "train_loss : 19.2508 test_loss : 17.573739844881842\n",
            "train_loss : 19.2408 test_loss : 17.56397935516106\n",
            "train_loss : 19.2308 test_loss : 17.554218893993397\n",
            "train_loss : 19.2208 test_loss : 17.5444584014173\n",
            "train_loss : 19.2108 test_loss : 17.534697964519793\n",
            "train_loss : 19.2008 test_loss : 17.52493747622667\n",
            "train_loss : 19.190804 test_loss : 17.515177013631348\n",
            "train_loss : 19.180801 test_loss : 17.505416519627598\n",
            "train_loss : 19.170801 test_loss : 17.495656084157748\n",
            "train_loss : 19.1608 test_loss : 17.485895597292274\n",
            "train_loss : 19.1508 test_loss : 17.476135134696957\n",
            "train_loss : 19.1408 test_loss : 17.466374640693203\n",
            "train_loss : 19.130802 test_loss : 17.45661420665101\n",
            "train_loss : 19.120802 test_loss : 17.446853718357882\n",
            "train_loss : 19.110802 test_loss : 17.437093255762566\n",
            "train_loss : 19.100801 test_loss : 17.427332761758812\n",
            "train_loss : 19.0908 test_loss : 17.41757232771662\n",
            "train_loss : 19.080801 test_loss : 17.40781183942349\n",
            "train_loss : 19.070803 test_loss : 17.398051377542004\n",
            "train_loss : 19.060802 test_loss : 17.388290882110592\n",
            "train_loss : 19.050802 test_loss : 17.378530452351367\n",
            "train_loss : 19.0408 test_loss : 17.36876995977527\n",
            "train_loss : 19.030802 test_loss : 17.35900950146292\n",
            "train_loss : 19.0208 test_loss : 17.3492490031762\n",
            "train_loss : 19.010801 test_loss : 17.33948857198932\n",
            "train_loss : 19.000801 test_loss : 17.329728077985568\n",
            "train_loss : 18.990803 test_loss : 17.319967621100876\n",
            "train_loss : 18.980803 test_loss : 17.31020711995884\n",
            "train_loss : 18.9708 test_loss : 17.300446693054926\n",
            "train_loss : 18.960802 test_loss : 17.29068644603569\n",
            "train_loss : 18.950802 test_loss : 17.280926220431297\n",
            "train_loss : 18.940804 test_loss : 17.271165971984402\n",
            "train_loss : 18.930801 test_loss : 17.26140574638001\n",
            "train_loss : 18.920803 test_loss : 17.251645497933115\n",
            "train_loss : 18.910805 test_loss : 17.241885272328723\n",
            "train_loss : 18.900803 test_loss : 17.232125023881828\n",
            "train_loss : 18.890804 test_loss : 17.222364798277436\n",
            "train_loss : 18.880804 test_loss : 17.21260454983054\n",
            "train_loss : 18.870804 test_loss : 17.20284432279849\n",
            "train_loss : 18.860802 test_loss : 17.1930840743516\n",
            "train_loss : 18.850805 test_loss : 17.18332385017486\n",
            "train_loss : 18.840803 test_loss : 17.173563601727967\n",
            "train_loss : 18.830805 test_loss : 17.163803376123575\n",
            "train_loss : 18.820807 test_loss : 17.154043129104338\n",
            "train_loss : 18.810804 test_loss : 17.144282902072288\n",
            "train_loss : 18.800806 test_loss : 17.134522652197738\n",
            "train_loss : 18.790808 test_loss : 17.124762426593342\n",
            "train_loss : 18.780806 test_loss : 17.11500217814645\n",
            "train_loss : 18.770805 test_loss : 17.105241953969713\n",
            "train_loss : 18.760807 test_loss : 17.09548170552282\n",
            "train_loss : 18.750807 test_loss : 17.085721479918426\n",
            "train_loss : 18.740807 test_loss : 17.07596123147153\n",
            "train_loss : 18.730806 test_loss : 17.066201007294794\n",
            "train_loss : 18.720806 test_loss : 17.056440758847902\n",
            "train_loss : 18.710808 test_loss : 17.046680533243507\n",
            "train_loss : 18.700808 test_loss : 17.03694209367215\n",
            "train_loss : 18.690807 test_loss : 17.027301625577273\n",
            "train_loss : 18.680809 test_loss : 17.017661134639898\n",
            "train_loss : 18.670809 test_loss : 17.00802066654502\n",
            "train_loss : 18.66081 test_loss : 16.998380175607643\n",
            "train_loss : 18.650808 test_loss : 16.988739707512767\n",
            "train_loss : 18.64081 test_loss : 16.979099216575392\n",
            "train_loss : 18.630829 test_loss : 16.969476071660388\n",
            "train_loss : 18.62094 test_loss : 16.95985288677101\n",
            "train_loss : 18.61105 test_loss : 16.950229697598665\n",
            "train_loss : 18.60116 test_loss : 16.94060654126241\n",
            "train_loss : 18.591269 test_loss : 16.930983386353816\n",
            "train_loss : 18.581377 test_loss : 16.921360187187876\n",
            "train_loss : 18.571487 test_loss : 16.911737003726156\n",
            "train_loss : 18.561598 test_loss : 16.902113858811152\n",
            "train_loss : 18.551706 test_loss : 16.892490713896148\n",
            "train_loss : 18.541815 test_loss : 16.882867509019587\n",
            "train_loss : 18.531925 test_loss : 16.87324437267052\n",
            "train_loss : 18.522034 test_loss : 16.863621197774737\n",
            "train_loss : 18.512144 test_loss : 16.85399800146411\n",
            "train_loss : 18.502253 test_loss : 16.844374849410826\n",
            "train_loss : 18.492365 test_loss : 16.834751707351135\n",
            "train_loss : 18.482473 test_loss : 16.825128488198008\n",
            "train_loss : 18.472582 test_loss : 16.815505333289412\n",
            "train_loss : 18.462692 test_loss : 16.805882174097846\n",
            "train_loss : 18.4528 test_loss : 16.796258992063784\n",
            "train_loss : 18.442911 test_loss : 16.786635827161593\n",
            "train_loss : 18.43302 test_loss : 16.77701267082534\n",
            "train_loss : 18.42313 test_loss : 16.76738948736362\n",
            "train_loss : 18.413239 test_loss : 16.75776631104018\n",
            "train_loss : 18.403347 test_loss : 16.748143161842208\n",
            "train_loss : 18.393457 test_loss : 16.738519989801734\n",
            "train_loss : 18.383568 test_loss : 16.728896824899547\n",
            "train_loss : 18.373676 test_loss : 16.71927366142501\n",
            "train_loss : 18.363787 test_loss : 16.709650446554853\n",
            "train_loss : 18.353897 test_loss : 16.70002731591641\n",
            "train_loss : 18.344006 test_loss : 16.69040412959938\n",
            "train_loss : 18.334114 test_loss : 16.68078096898016\n",
            "train_loss : 18.324223 test_loss : 16.671157791229064\n",
            "train_loss : 18.314333 test_loss : 16.661534647741714\n",
            "train_loss : 18.304443 test_loss : 16.651911464279994\n",
            "train_loss : 18.294554 test_loss : 16.642288290811866\n",
            "train_loss : 18.284662 test_loss : 16.632665131620303\n",
            "train_loss : 18.27477 test_loss : 16.62304195244155\n",
            "train_loss : 18.264881 test_loss : 16.61341879753295\n",
            "train_loss : 18.254992 test_loss : 16.603795628347793\n",
            "train_loss : 18.2451 test_loss : 16.594172463445602\n",
            "train_loss : 18.23521 test_loss : 16.58454928854982\n",
            "train_loss : 18.225319 test_loss : 16.57492611936466\n",
            "train_loss : 18.21543 test_loss : 16.565302937330596\n",
            "train_loss : 18.205538 test_loss : 16.55567977100075\n",
            "train_loss : 18.195646 test_loss : 16.546056596104968\n",
            "train_loss : 18.185755 test_loss : 16.536433435485744\n",
            "train_loss : 18.175865 test_loss : 16.526810256306995\n",
            "train_loss : 18.165977 test_loss : 16.517187104253708\n",
            "train_loss : 18.156086 test_loss : 16.50756393935152\n",
            "train_loss : 18.146193 test_loss : 16.497940753034488\n",
            "train_loss : 18.136305 test_loss : 16.48831758670464\n",
            "train_loss : 18.126415 test_loss : 16.478694431796043\n",
            "train_loss : 18.116524 test_loss : 16.469071239768386\n",
            "train_loss : 18.106632 test_loss : 16.45944808057682\n",
            "train_loss : 18.096743 test_loss : 16.449824934234158\n",
            "train_loss : 18.08685 test_loss : 16.440201745061813\n",
            "train_loss : 18.076962 test_loss : 16.430578554461814\n",
            "train_loss : 18.06707 test_loss : 16.420955403836185\n",
            "train_loss : 18.057178 test_loss : 16.411332228940402\n",
            "train_loss : 18.047289 test_loss : 16.401709081170086\n",
            "train_loss : 18.037401 test_loss : 16.39208590341899\n",
            "train_loss : 18.027508 test_loss : 16.382462732806175\n",
            "train_loss : 18.017616 test_loss : 16.37283952079133\n",
            "train_loss : 18.007727 test_loss : 16.363216374448672\n",
            "train_loss : 17.997835 test_loss : 16.35359321668476\n",
            "train_loss : 17.987946 test_loss : 16.34397006320382\n",
            "train_loss : 17.978056 test_loss : 16.33434689116335\n",
            "train_loss : 17.968164 test_loss : 16.324723724833504\n",
            "train_loss : 17.958273 test_loss : 16.315100528522876\n",
            "train_loss : 17.948385 test_loss : 16.30547739217381\n",
            "train_loss : 17.938494 test_loss : 16.29585421870568\n",
            "train_loss : 17.928602 test_loss : 16.286231019539745\n",
            "train_loss : 17.918713 test_loss : 16.276607877480053\n",
            "train_loss : 17.908821 test_loss : 16.266984704011925\n",
            "train_loss : 17.89893 test_loss : 16.257361529116142\n",
            "train_loss : 17.88904 test_loss : 16.247738362786297\n",
            "train_loss : 17.87915 test_loss : 16.238115207877698\n",
            "train_loss : 17.869259 test_loss : 16.228492024415978\n",
            "train_loss : 17.85937 test_loss : 16.21886883381598\n",
            "train_loss : 17.84948 test_loss : 16.209245677479725\n",
            "train_loss : 17.839588 test_loss : 16.199622468320193\n",
            "train_loss : 17.829697 test_loss : 16.189999340537064\n",
            "train_loss : 17.819807 test_loss : 16.180376164213623\n",
            "train_loss : 17.809916 test_loss : 16.170753017870965\n",
            "train_loss : 17.800026 test_loss : 16.161129818705025\n",
            "train_loss : 17.790136 test_loss : 16.151506638098617\n",
            "train_loss : 17.780243 test_loss : 16.141883494611267\n",
            "train_loss : 17.770353 test_loss : 16.13226031400486\n",
            "train_loss : 17.760464 test_loss : 16.122637131970798\n",
            "train_loss : 17.750574 test_loss : 16.113013961357982\n",
            "train_loss : 17.740685 test_loss : 16.10339081929829\n",
            "train_loss : 17.730791 test_loss : 16.093767641547196\n",
            "train_loss : 17.7209 test_loss : 16.08414445951313\n",
            "train_loss : 17.711012 test_loss : 16.0745213088875\n",
            "train_loss : 17.70112 test_loss : 16.064898106866252\n",
            "train_loss : 17.69123 test_loss : 16.05527497622781\n",
            "train_loss : 17.68134 test_loss : 16.045651784200153\n",
            "train_loss : 17.67145 test_loss : 16.036028610732025\n",
            "train_loss : 17.661558 test_loss : 16.02640546153405\n",
            "train_loss : 17.651669 test_loss : 16.01678227949999\n",
            "train_loss : 17.641813 test_loss : 16.00719616227521\n",
            "train_loss : 17.632048 test_loss : 15.997638502520713\n",
            "train_loss : 17.622334 test_loss : 15.988080845621528\n",
            "train_loss : 17.612625 test_loss : 15.978523193005314\n",
            "train_loss : 17.602911 test_loss : 15.968965520401913\n",
            "train_loss : 17.5932 test_loss : 15.959407813534762\n",
            "train_loss : 17.583487 test_loss : 15.949850185188701\n",
            "train_loss : 17.573774 test_loss : 15.940292519723581\n",
            "train_loss : 17.564062 test_loss : 15.93073484712018\n",
            "train_loss : 17.55435 test_loss : 15.921177187365684\n",
            "train_loss : 17.544636 test_loss : 15.911619509051658\n",
            "train_loss : 17.534924 test_loss : 15.902061837875912\n",
            "train_loss : 17.525213 test_loss : 15.89250419525329\n",
            "train_loss : 17.5155 test_loss : 15.882946511228639\n",
            "train_loss : 17.505787 test_loss : 15.873388834342268\n",
            "train_loss : 17.496075 test_loss : 15.863831158883555\n",
            "train_loss : 17.486362 test_loss : 15.854273510550307\n",
            "train_loss : 17.47665 test_loss : 15.84471585507878\n",
            "train_loss : 17.466938 test_loss : 15.835158195324285\n",
            "train_loss : 17.457224 test_loss : 15.825600525576196\n",
            "train_loss : 17.447515 test_loss : 15.816042837268576\n",
            "train_loss : 17.437803 test_loss : 15.806485178941735\n",
            "train_loss : 17.42809 test_loss : 15.79692752632552\n",
            "train_loss : 17.418375 test_loss : 15.787369887985866\n",
            "train_loss : 17.408665 test_loss : 15.77781219111231\n",
            "train_loss : 17.398952 test_loss : 15.768254509942972\n",
            "train_loss : 17.389238 test_loss : 15.758696861609724\n",
            "train_loss : 17.379526 test_loss : 15.749139189006323\n",
            "train_loss : 17.369814 test_loss : 15.739581496415735\n",
            "train_loss : 17.360104 test_loss : 15.730023853793114\n",
            "train_loss : 17.35039 test_loss : 15.720466184045023\n",
            "train_loss : 17.340677 test_loss : 15.710908522862871\n",
            "train_loss : 17.330965 test_loss : 15.701350843121189\n",
            "train_loss : 17.321253 test_loss : 15.691793169090131\n",
            "train_loss : 17.311539 test_loss : 15.682235517901573\n",
            "train_loss : 17.301828 test_loss : 15.672677845298171\n",
            "train_loss : 17.292114 test_loss : 15.66312021695211\n",
            "train_loss : 17.282404 test_loss : 15.653562534355116\n",
            "train_loss : 17.272692 test_loss : 15.644004866034683\n",
            "train_loss : 17.262978 test_loss : 15.634447173444094\n",
            "train_loss : 17.253267 test_loss : 15.624889542242721\n",
            "train_loss : 17.243555 test_loss : 15.615331873922289\n",
            "train_loss : 17.23384 test_loss : 15.605774205601856\n",
            "train_loss : 17.224129 test_loss : 15.596216535853765\n",
            "train_loss : 17.214417 test_loss : 15.586658860395051\n",
            "train_loss : 17.204704 test_loss : 15.57710120635118\n",
            "train_loss : 17.194992 test_loss : 15.56754354088606\n",
            "train_loss : 17.185278 test_loss : 15.557985888269844\n",
            "train_loss : 17.175566 test_loss : 15.548428222804723\n",
            "train_loss : 17.165854 test_loss : 15.538870531641791\n",
            "train_loss : 17.156141 test_loss : 15.529312923282918\n",
            "train_loss : 17.14643 test_loss : 15.51975521927108\n",
            "train_loss : 17.136719 test_loss : 15.510197553805959\n",
            "train_loss : 17.127005 test_loss : 15.500639888340839\n",
            "train_loss : 17.117292 test_loss : 15.491082224303376\n",
            "train_loss : 17.10758 test_loss : 15.481524534568102\n",
            "train_loss : 17.09787 test_loss : 15.471966904794385\n",
            "train_loss : 17.088156 test_loss : 15.46240922219739\n",
            "train_loss : 17.078444 test_loss : 15.452851555304614\n",
            "train_loss : 17.068731 test_loss : 15.443293881273556\n",
            "train_loss : 17.059021 test_loss : 15.43373620295953\n",
            "train_loss : 17.049307 test_loss : 15.424178544632689\n",
            "train_loss : 17.039595 test_loss : 15.414620880595225\n",
            "train_loss : 17.029882 test_loss : 15.405063189432292\n",
            "train_loss : 17.02017 test_loss : 15.395505548237326\n",
            "train_loss : 17.010456 test_loss : 15.385947909897673\n",
            "train_loss : 17.000744 test_loss : 15.37639024871552\n",
            "train_loss : 16.991032 test_loss : 15.36683255326962\n",
            "train_loss : 16.98132 test_loss : 15.35727490208106\n",
            "train_loss : 16.971607 test_loss : 15.347717220911722\n",
            "train_loss : 16.961895 test_loss : 15.338159586855037\n",
            "train_loss : 16.952206 test_loss : 15.328632660254748\n",
            "train_loss : 16.942562 test_loss : 15.319105800754295\n",
            "train_loss : 16.932915 test_loss : 15.309578922694314\n",
            "train_loss : 16.92327 test_loss : 15.300052036068397\n",
            "train_loss : 16.913624 test_loss : 15.290525187989195\n",
            "train_loss : 16.903976 test_loss : 15.280998305646245\n",
            "train_loss : 16.89433 test_loss : 15.271471363341737\n",
            "train_loss : 16.884684 test_loss : 15.26194457665175\n",
            "train_loss : 16.875046 test_loss : 15.252451385566575\n",
            "train_loss : 16.865454 test_loss : 15.242958205902648\n",
            "train_loss : 16.85586 test_loss : 15.233465064785438\n",
            "train_loss : 16.846266 test_loss : 15.223971929378852\n",
            "train_loss : 16.836674 test_loss : 15.214478741148989\n",
            "train_loss : 16.827082 test_loss : 15.20498557718928\n",
            "train_loss : 16.8175 test_loss : 15.19553064443394\n",
            "train_loss : 16.807932 test_loss : 15.186075724527507\n",
            "train_loss : 16.798367 test_loss : 15.176620744659516\n",
            "train_loss : 16.7888 test_loss : 15.167165783351052\n",
            "train_loss : 16.77923 test_loss : 15.157710846312746\n",
            "train_loss : 16.769665 test_loss : 15.14825591212975\n",
            "train_loss : 16.760098 test_loss : 15.138800949393632\n",
            "train_loss : 16.75053 test_loss : 15.129345983802201\n",
            "train_loss : 16.740961 test_loss : 15.11989104105327\n",
            "train_loss : 16.731396 test_loss : 15.11043609687668\n",
            "train_loss : 16.721828 test_loss : 15.100981138423531\n",
            "train_loss : 16.712261 test_loss : 15.091526179970382\n",
            "train_loss : 16.702694 test_loss : 15.082071242932074\n",
            "train_loss : 16.693127 test_loss : 15.072616295900174\n",
            "train_loss : 16.683582 test_loss : 15.063186469906105\n",
            "train_loss : 16.674063 test_loss : 15.05375660108235\n",
            "train_loss : 16.664543 test_loss : 15.044326757956409\n",
            "train_loss : 16.655025 test_loss : 15.034896931962338\n",
            "train_loss : 16.645506 test_loss : 15.025467107395926\n",
            "train_loss : 16.635988 test_loss : 15.016037225723267\n",
            "train_loss : 16.626469 test_loss : 15.006607419716383\n",
            "train_loss : 16.616951 test_loss : 14.997177560886223\n",
            "train_loss : 16.607431 test_loss : 14.987747704911374\n",
            "train_loss : 16.597912 test_loss : 14.978317867496056\n",
            "train_loss : 16.588394 test_loss : 14.968888038646675\n",
            "train_loss : 16.578875 test_loss : 14.959458196948388\n",
            "train_loss : 16.569357 test_loss : 14.950028338118228\n",
            "train_loss : 16.559837 test_loss : 14.940598503558222\n",
            "train_loss : 16.55032 test_loss : 14.931168688985402\n",
            "train_loss : 16.5408 test_loss : 14.92173882301696\n",
            "train_loss : 16.531282 test_loss : 14.912308991312267\n",
            "train_loss : 16.521765 test_loss : 14.902879146758668\n",
            "train_loss : 16.512245 test_loss : 14.893449319336943\n",
            "train_loss : 16.502726 test_loss : 14.884019430526003\n",
            "train_loss : 16.493208 test_loss : 14.874589628802088\n",
            "train_loss : 16.48369 test_loss : 14.865159769971928\n",
            "train_loss : 16.47417 test_loss : 14.85572993969489\n",
            "train_loss : 16.464651 test_loss : 14.846300105134883\n",
            "train_loss : 16.455133 test_loss : 14.836870283423783\n",
            "train_loss : 16.445614 test_loss : 14.827440406034093\n",
            "train_loss : 16.436096 test_loss : 14.818010585750649\n",
            "train_loss : 16.426577 test_loss : 14.80858074119705\n",
            "train_loss : 16.417059 test_loss : 14.799150893788138\n",
            "train_loss : 16.40754 test_loss : 14.789721027819697\n",
            "train_loss : 16.398022 test_loss : 14.780291213246876\n",
            "train_loss : 16.3885 test_loss : 14.770861375831558\n",
            "train_loss : 16.378986 test_loss : 14.761431509863117\n",
            "train_loss : 16.369465 test_loss : 14.752001651032955\n",
            "train_loss : 16.359945 test_loss : 14.742571849309043\n",
            "train_loss : 16.350428 test_loss : 14.733141997617162\n",
            "train_loss : 16.34091 test_loss : 14.72371215020825\n",
            "train_loss : 16.331388 test_loss : 14.714282324214182\n",
            "train_loss : 16.32187 test_loss : 14.704852482515895\n",
            "train_loss : 16.312353 test_loss : 14.69542263225167\n",
            "train_loss : 16.302834 test_loss : 14.68599277770448\n",
            "train_loss : 16.293314 test_loss : 14.67656297740822\n",
            "train_loss : 16.283796 test_loss : 14.667133114295092\n",
            "train_loss : 16.274279 test_loss : 14.65770329686696\n",
            "train_loss : 16.264757 test_loss : 14.648273478011172\n",
            "train_loss : 16.25524 test_loss : 14.638843564930076\n",
            "train_loss : 16.24572 test_loss : 14.6294137489296\n",
            "train_loss : 16.236202 test_loss : 14.619983960054592\n",
            "train_loss : 16.226683 test_loss : 14.61055409836912\n",
            "train_loss : 16.217165 test_loss : 14.601124239538958\n",
            "train_loss : 16.207647 test_loss : 14.591694392130046\n",
            "train_loss : 16.198128 test_loss : 14.582264571846602\n",
            "train_loss : 16.188608 test_loss : 14.572834695884568\n",
            "train_loss : 16.17909 test_loss : 14.563404887022372\n",
            "train_loss : 16.16957 test_loss : 14.553975016770963\n",
            "train_loss : 16.160053 test_loss : 14.544545215047048\n",
            "train_loss : 16.150536 test_loss : 14.53511534051267\n",
            "train_loss : 16.141016 test_loss : 14.525685528795162\n",
            "train_loss : 16.131498 test_loss : 14.516255664254377\n",
            "train_loss : 16.121979 test_loss : 14.506825829694371\n",
            "train_loss : 16.112461 test_loss : 14.497395997989678\n",
            "train_loss : 16.102942 test_loss : 14.487966173423265\n",
            "train_loss : 16.093424 test_loss : 14.478536303171854\n",
            "train_loss : 16.083904 test_loss : 14.469106454335288\n",
            "train_loss : 16.074387 test_loss : 14.459676608354032\n",
            "train_loss : 16.064867 test_loss : 14.450246785215276\n",
            "train_loss : 16.055347 test_loss : 14.440816930668083\n",
            "train_loss : 16.045828 test_loss : 14.43138707612089\n",
            "train_loss : 16.03631 test_loss : 14.421957230139636\n",
            "train_loss : 16.026793 test_loss : 14.412527399862597\n",
            "train_loss : 16.017273 test_loss : 14.403097596711028\n",
            "train_loss : 16.007755 test_loss : 14.393667730742585\n",
            "train_loss : 15.998236 test_loss : 14.384237890471955\n",
            "train_loss : 15.988721 test_loss : 14.374837843957776\n",
            "train_loss : 15.979205 test_loss : 14.365437800298908\n",
            "train_loss : 15.969696 test_loss : 14.356007948607028\n",
            "train_loss : 15.960184 test_loss : 14.346607962054406\n",
            "train_loss : 15.950672 test_loss : 14.337207942665694\n",
            "train_loss : 15.94116 test_loss : 14.327807926132294\n",
            "train_loss : 15.931648 test_loss : 14.318407875335145\n",
            "train_loss : 15.922135 test_loss : 14.309007821682684\n",
            "train_loss : 15.912625 test_loss : 14.299577972846118\n",
            "train_loss : 15.903114 test_loss : 14.290177997714745\n",
            "train_loss : 15.8936 test_loss : 14.280777942634629\n",
            "train_loss : 15.88409 test_loss : 14.271377900403417\n",
            "train_loss : 15.8745775 test_loss : 14.261977829619083\n",
            "train_loss : 15.865063 test_loss : 14.252577848777086\n",
            "train_loss : 15.855553 test_loss : 14.2431778122565\n",
            "train_loss : 15.846042 test_loss : 14.233747953426338\n",
            "train_loss : 15.83653 test_loss : 14.224347931182313\n",
            "train_loss : 15.827019 test_loss : 14.214947943202036\n",
            "train_loss : 15.817507 test_loss : 14.205547885266606\n",
            "train_loss : 15.807993 test_loss : 14.196147865877894\n",
            "train_loss : 15.798481 test_loss : 14.186747856482775\n",
            "train_loss : 15.78897 test_loss : 14.177317981948395\n",
            "train_loss : 15.77946 test_loss : 14.16791797112562\n",
            "train_loss : 15.769949 test_loss : 14.158517951736908\n",
            "train_loss : 15.760435 test_loss : 14.149117895229134\n",
            "train_loss : 15.750923 test_loss : 14.139717855853235\n",
            "train_loss : 15.741411 test_loss : 14.130317865017645\n",
            "train_loss : 15.731899 test_loss : 14.120917812792841\n",
            "train_loss : 15.72239 test_loss : 14.111523292735665\n",
            "train_loss : 15.712877 test_loss : 14.102175342822504\n",
            "train_loss : 15.703364 test_loss : 14.092827421462465\n",
            "train_loss : 15.6938505 test_loss : 14.083479488681176\n",
            "train_loss : 15.68434 test_loss : 14.074131510214892\n",
            "train_loss : 15.674828 test_loss : 14.064783633112194\n",
            "train_loss : 15.665316 test_loss : 14.055415741697757\n",
            "train_loss : 15.655807 test_loss : 14.046067828903656\n",
            "train_loss : 15.646295 test_loss : 14.036719870424557\n",
            "train_loss : 15.636782 test_loss : 14.027371943353893\n",
            "train_loss : 15.627269 test_loss : 14.018024019138542\n",
            "train_loss : 15.617757 test_loss : 14.008676077791316\n",
            "train_loss : 15.608244 test_loss : 13.999308200653442\n",
            "train_loss : 15.598735 test_loss : 13.989960290714652\n",
            "train_loss : 15.589223 test_loss : 13.9806123893418\n",
            "train_loss : 15.579711 test_loss : 13.971264465126449\n",
            "train_loss : 15.570197 test_loss : 13.961916489515476\n",
            "train_loss : 15.560685 test_loss : 13.952568528181065\n",
            "train_loss : 15.551173 test_loss : 13.943220618242275\n",
            "train_loss : 15.541662 test_loss : 13.933852723972526\n",
            "train_loss : 15.532151 test_loss : 13.924504825454987\n",
            "train_loss : 15.522639 test_loss : 13.915156886963073\n",
            "train_loss : 15.513126 test_loss : 13.905808957037097\n",
            "train_loss : 15.503613 test_loss : 13.89646101283456\n",
            "train_loss : 15.494102 test_loss : 13.887113094329834\n",
            "train_loss : 15.4845915 test_loss : 13.87774523146852\n",
            "train_loss : 15.4750805 test_loss : 13.868397312963793\n",
            "train_loss : 15.4655695 test_loss : 13.85904934877407\n",
            "train_loss : 15.456055 test_loss : 13.849701410282158\n",
            "train_loss : 15.446543 test_loss : 13.840353483211494\n",
            "train_loss : 15.437031 test_loss : 13.83100555614083\n",
            "train_loss : 15.427518 test_loss : 13.821657640491416\n",
            "train_loss : 15.418009 test_loss : 13.812289789051352\n",
            "train_loss : 15.408497 test_loss : 13.802941816295693\n",
            "train_loss : 15.398983 test_loss : 13.793593880659092\n",
            "train_loss : 15.389471 test_loss : 13.784245962154365\n",
            "train_loss : 15.379959 test_loss : 13.774898049360264\n",
            "train_loss : 15.370448 test_loss : 13.765550128000225\n",
            "train_loss : 15.360938 test_loss : 13.75618228798141\n",
            "train_loss : 15.351427 test_loss : 13.746834306659812\n",
            "train_loss : 15.341912 test_loss : 13.737486410997585\n",
            "train_loss : 15.3324 test_loss : 13.728138418254739\n",
            "train_loss : 15.322887 test_loss : 13.718790531158447\n",
            "train_loss : 15.313377 test_loss : 13.70944258695591\n",
            "train_loss : 15.303865 test_loss : 13.700094668451184\n",
            "train_loss : 15.294354 test_loss : 13.690726808445183\n",
            "train_loss : 15.2848425 test_loss : 13.681378867097957\n",
            "train_loss : 15.275329 test_loss : 13.672030905763545\n",
            "train_loss : 15.265818 test_loss : 13.662683010101318\n",
            "train_loss : 15.256308 test_loss : 13.653335034490345\n",
            "train_loss : 15.246796 test_loss : 13.643987127406868\n",
            "train_loss : 15.237283 test_loss : 13.634619258834931\n",
            "train_loss : 15.227771 test_loss : 13.625271340330205\n",
            "train_loss : 15.218259 test_loss : 13.615923381851104\n",
            "train_loss : 15.208747 test_loss : 13.606575457635754\n",
            "train_loss : 15.199235 test_loss : 13.59722749344603\n",
            "train_loss : 15.189724 test_loss : 13.587879597783802\n",
            "train_loss : 15.180212 test_loss : 13.578511720645928\n",
            "train_loss : 15.170703 test_loss : 13.569163782154014\n",
            "train_loss : 15.161187 test_loss : 13.559815857938665\n",
            "train_loss : 15.151677 test_loss : 13.550467916591439\n",
            "train_loss : 15.142165 test_loss : 13.541119986665462\n",
            "train_loss : 15.132651 test_loss : 13.531772042462926\n",
            "train_loss : 15.12314 test_loss : 13.52242411253695\n",
            "train_loss : 15.113628 test_loss : 13.513056263952198\n",
            "train_loss : 15.104117 test_loss : 13.503708322604973\n",
            "train_loss : 15.094603 test_loss : 13.494360381257748\n",
            "train_loss : 15.0850935 test_loss : 13.485012439910523\n",
            "train_loss : 15.075582 test_loss : 13.475664558524857\n",
            "train_loss : 15.066072 test_loss : 13.466287181762878\n",
            "train_loss : 15.05656 test_loss : 13.45693924898159\n",
            "train_loss : 15.047048 test_loss : 13.447561903628047\n",
            "train_loss : 15.03754 test_loss : 13.438213979412696\n",
            "train_loss : 15.028028 test_loss : 13.42886601236766\n",
            "train_loss : 15.018518 test_loss : 13.419488658448179\n",
            "train_loss : 15.009009 test_loss : 13.41014072281158\n",
            "train_loss : 14.999497 test_loss : 13.400793246880262\n",
            "train_loss : 14.989988 test_loss : 13.391415444676747\n",
            "train_loss : 14.980477 test_loss : 13.38206750047421\n",
            "train_loss : 14.9709635 test_loss : 13.37271960195667\n",
            "train_loss : 14.961454 test_loss : 13.363341774055344\n",
            "train_loss : 14.951944 test_loss : 13.353993881248428\n",
            "train_loss : 14.942433 test_loss : 13.344616013372729\n",
            "train_loss : 14.932924 test_loss : 13.335268083446755\n",
            "train_loss : 14.923412 test_loss : 13.32592015637609\n",
            "train_loss : 14.913901 test_loss : 13.316542348461951\n",
            "train_loss : 14.904392 test_loss : 13.30719444994441\n",
            "train_loss : 14.894881 test_loss : 13.29784647718875\n",
            "train_loss : 14.88537 test_loss : 13.288468677840546\n",
            "train_loss : 14.87586 test_loss : 13.279120693663637\n",
            "train_loss : 14.866347 test_loss : 13.269772815133283\n",
            "train_loss : 14.856838 test_loss : 13.260395055759453\n",
            "train_loss : 14.847326 test_loss : 13.251047077293167\n",
            "train_loss : 14.837814 test_loss : 13.241669257957778\n",
            "train_loss : 14.828306 test_loss : 13.232321330887116\n",
            "train_loss : 14.818794 test_loss : 13.22297340096114\n",
            "train_loss : 14.809283 test_loss : 13.213595647297934\n",
            "train_loss : 14.799773 test_loss : 13.204247663121023\n",
            "train_loss : 14.790262 test_loss : 13.194899744616297\n",
            "train_loss : 14.780753 test_loss : 13.185521885306535\n",
            "train_loss : 14.771241 test_loss : 13.176174003920869\n",
            "train_loss : 14.76173 test_loss : 13.166826054007707\n",
            "train_loss : 14.752221 test_loss : 13.157448234672318\n",
            "train_loss : 14.74271 test_loss : 13.148100287614469\n",
            "train_loss : 14.733198 test_loss : 13.138722499687514\n",
            "train_loss : 14.723689 test_loss : 13.129374626867785\n",
            "train_loss : 14.714177 test_loss : 13.120026631269626\n",
            "train_loss : 14.704668 test_loss : 13.110648854763921\n",
            "train_loss : 14.695155 test_loss : 13.101300924837947\n",
            "train_loss : 14.685643 test_loss : 13.091952972069471\n",
            "train_loss : 14.676134 test_loss : 13.082575164155331\n",
            "train_loss : 14.666623 test_loss : 13.073227254216542\n",
            "train_loss : 14.657111 test_loss : 13.063849434881153\n",
            "train_loss : 14.647602 test_loss : 13.054501484967991\n",
            "train_loss : 14.638091 test_loss : 13.04515354933139\n",
            "train_loss : 14.628581 test_loss : 13.035775735706626\n",
            "train_loss : 14.619073 test_loss : 13.0264278172019\n",
            "train_loss : 14.609559 test_loss : 13.017109702447218\n",
            "train_loss : 14.600086 test_loss : 13.007761669729998\n",
            "train_loss : 14.590611 test_loss : 12.998413668421215\n",
            "train_loss : 14.581139 test_loss : 12.989065621427434\n",
            "train_loss : 14.571665 test_loss : 12.979747532370562\n",
            "train_loss : 14.562193 test_loss : 12.970399491087406\n",
            "train_loss : 14.552718 test_loss : 12.96105148692331\n",
            "train_loss : 14.543243 test_loss : 12.951703505601712\n",
            "train_loss : 14.533771 test_loss : 12.942385325174845\n",
            "train_loss : 14.524298 test_loss : 12.933037312444812\n",
            "train_loss : 14.514826 test_loss : 12.923689331123215\n",
            "train_loss : 14.505351 test_loss : 12.914341295550683\n",
            "train_loss : 14.495877 test_loss : 12.904993317084399\n",
            "train_loss : 14.486404 test_loss : 12.895675136657532\n",
            "train_loss : 14.47693 test_loss : 12.886327132493436\n",
            "train_loss : 14.467457 test_loss : 12.876979119763403\n",
            "train_loss : 14.457982 test_loss : 12.867631112743995\n",
            "train_loss : 14.44851 test_loss : 12.85831294659369\n",
            "train_loss : 14.439041 test_loss : 12.848944575486783\n",
            "train_loss : 14.429565 test_loss : 12.839626455021476\n",
            "train_loss : 14.4200945 test_loss : 12.830258081059256\n",
            "train_loss : 14.410622 test_loss : 12.820939894921766\n",
            "train_loss : 14.401148 test_loss : 12.811571529525482\n",
            "train_loss : 14.391678 test_loss : 12.802253437613299\n",
            "train_loss : 14.382203 test_loss : 12.792891493814434\n",
            "train_loss : 14.372736 test_loss : 12.783655654884384\n",
            "train_loss : 14.363259 test_loss : 12.774361930207577\n",
            "train_loss : 14.353784 test_loss : 12.765068248360457\n",
            "train_loss : 14.344312 test_loss : 12.755785248236743\n",
            "train_loss : 14.334839 test_loss : 12.746491509283373\n",
            "train_loss : 14.325368 test_loss : 12.737255661787387\n",
            "train_loss : 14.315901 test_loss : 12.72791479304879\n",
            "train_loss : 14.306421 test_loss : 12.718678936986866\n",
            "train_loss : 14.296954 test_loss : 12.70933808252483\n",
            "train_loss : 14.287479 test_loss : 12.700102203620409\n",
            "train_loss : 14.2780075 test_loss : 12.690761300618064\n",
            "train_loss : 14.268536 test_loss : 12.681525487385825\n",
            "train_loss : 14.259063 test_loss : 12.672184595804728\n",
            "train_loss : 14.249593 test_loss : 12.662948759729991\n",
            "train_loss : 14.240118 test_loss : 12.653607879570144\n",
            "train_loss : 14.230646 test_loss : 12.644372026363532\n",
            "train_loss : 14.221169 test_loss : 12.63507830454204\n",
            "train_loss : 14.211698 test_loss : 12.62573738440782\n",
            "train_loss : 14.202232 test_loss : 12.616501605439328\n",
            "train_loss : 14.192753 test_loss : 12.607265746522092\n",
            "train_loss : 14.183287 test_loss : 12.597924812111312\n",
            "train_loss : 14.173806 test_loss : 12.58868899602376\n",
            "train_loss : 14.164342 test_loss : 12.579348093021416\n",
            "train_loss : 14.154863 test_loss : 12.570112242670117\n",
            "train_loss : 14.145395 test_loss : 12.56077137393152\n",
            "train_loss : 14.13592 test_loss : 12.551535489316471\n",
            "train_loss : 14.126451 test_loss : 12.542194594880064\n",
            "train_loss : 14.116977 test_loss : 12.532958807345636\n",
            "train_loss : 14.107503 test_loss : 12.52361790148798\n",
            "train_loss : 14.098045 test_loss : 12.514424315469707\n",
            "train_loss : 14.08858 test_loss : 12.505125639681332\n",
            "train_loss : 14.0791235 test_loss : 12.495827012433264\n",
            "train_loss : 14.069666 test_loss : 12.486528330934261\n",
            "train_loss : 14.060208 test_loss : 12.477287520905454\n",
            "train_loss : 14.050754 test_loss : 12.46798888509145\n",
            "train_loss : 14.041298 test_loss : 12.458690169328701\n",
            "train_loss : 14.031837 test_loss : 12.449391547791258\n",
            "train_loss : 14.022385 test_loss : 12.440150754894326\n",
            "train_loss : 14.012927 test_loss : 12.430852076250636\n",
            "train_loss : 14.0034685 test_loss : 12.421553446147255\n",
            "train_loss : 13.994014 test_loss : 12.412312636118449\n",
            "train_loss : 13.984558 test_loss : 12.40301398317257\n",
            "train_loss : 13.9751005 test_loss : 12.393715353069192\n",
            "train_loss : 13.965643 test_loss : 12.384474528763823\n",
            "train_loss : 13.956187 test_loss : 12.375175875817945\n",
            "train_loss : 13.94673 test_loss : 12.36587725142519\n",
            "train_loss : 13.937274 test_loss : 12.356636424264508\n",
            "train_loss : 13.927818 test_loss : 12.34733774847613\n",
            "train_loss : 13.918362 test_loss : 12.338039121228064\n",
            "train_loss : 13.908903 test_loss : 12.32879827979082\n",
            "train_loss : 13.899448 test_loss : 12.319499663964002\n",
            "train_loss : 13.889991 test_loss : 12.31020101672875\n",
            "train_loss : 13.880534 test_loss : 12.300902335229749\n",
            "train_loss : 13.871079 test_loss : 12.291661556609377\n",
            "train_loss : 13.861622 test_loss : 12.282362886531624\n",
            "train_loss : 13.852163 test_loss : 12.273064227875121\n",
            "train_loss : 13.8427105 test_loss : 12.263823432122877\n",
            "train_loss : 13.833253 test_loss : 12.254524724926064\n",
            "train_loss : 13.823795 test_loss : 12.245226123375806\n",
            "train_loss : 13.81434 test_loss : 12.235985350466061\n",
            "train_loss : 13.804882 test_loss : 12.226686680388308\n",
            "train_loss : 13.795424 test_loss : 12.217388030297743\n",
            "train_loss : 13.78597 test_loss : 12.208147180294562\n",
            "train_loss : 13.776513 test_loss : 12.198848530203996\n",
            "train_loss : 13.767056 test_loss : 12.18954986869218\n",
            "train_loss : 13.757599 test_loss : 12.180309027254939\n",
            "train_loss : 13.748143 test_loss : 12.17101040857281\n",
            "train_loss : 13.738686 test_loss : 12.16171176704818\n",
            "train_loss : 13.729229 test_loss : 12.152471022691557\n",
            "train_loss : 13.719774 test_loss : 12.143172306928808\n",
            "train_loss : 13.710317 test_loss : 12.133873659693554\n",
            "train_loss : 13.700859 test_loss : 12.124574983905177\n",
            "train_loss : 13.691404 test_loss : 12.11533421099543\n",
            "train_loss : 13.681948 test_loss : 12.106035598023924\n",
            "train_loss : 13.67249 test_loss : 12.096736882261174\n",
            "train_loss : 13.663034 test_loss : 12.08749609793018\n",
            "train_loss : 13.653577 test_loss : 12.078197467826799\n",
            "train_loss : 13.644121 test_loss : 12.06889875206405\n",
            "train_loss : 13.634665 test_loss : 12.059657973443677\n",
            "train_loss : 13.625209 test_loss : 12.050359326208424\n",
            "train_loss : 13.615751 test_loss : 12.041060664696609\n",
            "train_loss : 13.606294 test_loss : 12.031819843246552\n",
            "train_loss : 13.596837 test_loss : 12.022521213143174\n",
            "train_loss : 13.58738 test_loss : 12.013222548776046\n",
            "train_loss : 13.577925 test_loss : 12.003981807274734\n",
            "train_loss : 13.568469 test_loss : 11.994683128631046\n",
            "train_loss : 13.559011 test_loss : 11.985384501382978\n",
            "train_loss : 13.549556 test_loss : 11.976143642813861\n",
            "train_loss : 13.540101 test_loss : 11.966844947038298\n",
            "train_loss : 13.5306425 test_loss : 11.957546342632728\n",
            "train_loss : 13.521185 test_loss : 11.94824768968685\n",
            "train_loss : 13.511729 test_loss : 11.939006853960233\n",
            "train_loss : 13.502273 test_loss : 11.929708260975913\n",
            "train_loss : 13.492816 test_loss : 11.920409570910975\n",
            "train_loss : 13.483361 test_loss : 11.911168795145915\n",
            "train_loss : 13.473904 test_loss : 11.901870113646915\n",
            "train_loss : 13.464446 test_loss : 11.892571440713851\n",
            "train_loss : 13.4549885 test_loss : 11.883330667804104\n",
            "train_loss : 13.445533 test_loss : 11.874032043411347\n",
            "train_loss : 13.436075 test_loss : 11.864733350491097\n",
            "train_loss : 13.42662 test_loss : 11.855492571870725\n",
            "train_loss : 13.417164 test_loss : 11.846193913214222\n",
            "train_loss : 13.407706 test_loss : 11.836895240281157\n",
            "train_loss : 13.398249 test_loss : 11.82765447879266\n",
            "train_loss : 13.388796 test_loss : 11.818355797293657\n",
            "train_loss : 13.379335 test_loss : 11.809057135781842\n",
            "train_loss : 13.36988 test_loss : 11.799816377148657\n",
            "train_loss : 13.360425 test_loss : 11.790517692794342\n",
            "train_loss : 13.350966 test_loss : 11.781219082678149\n",
            "train_loss : 13.34151 test_loss : 11.771920398323836\n",
            "train_loss : 13.332056 test_loss : 11.762679594005654\n",
            "train_loss : 13.322598 test_loss : 11.753380966757586\n",
            "train_loss : 13.31314 test_loss : 11.744082290969208\n",
            "train_loss : 13.303683 test_loss : 11.73484147808509\n",
            "train_loss : 13.2942295 test_loss : 11.725542788020151\n",
            "train_loss : 13.28477 test_loss : 11.716244166482708\n",
            "train_loss : 13.275314 test_loss : 11.707003353598589\n",
            "train_loss : 13.26586 test_loss : 11.69770471207396\n",
            "train_loss : 13.256402 test_loss : 11.688406073404643\n",
            "train_loss : 13.246945 test_loss : 11.67916528050771\n",
            "train_loss : 13.237491 test_loss : 11.669866627561833\n",
            "train_loss : 13.22803 test_loss : 11.660567974615955\n",
            "train_loss : 13.218575 test_loss : 11.65132714459996\n",
            "train_loss : 13.20912 test_loss : 11.642028505930643\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2u32IQUdYX-4"
      },
      "source": [
        "Visualisation de l'évolution du coût pendant l'entraînement. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "03l20groTpLh",
        "outputId": "e7743dc3-96f7-4196-d92d-5abfc636a5f0"
      },
      "source": [
        "plt.plot(np.arange(epoch), history_train, label='train loss')\n",
        "plt.plot(np.arange(epoch), history_test, label='test loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('MAE')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pKkixYFY1qd"
      },
      "source": [
        "## Sauvegarde du modèle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mM_7mFShZ906"
      },
      "source": [
        "Maintenant que notre modèle est entraîné, il est temps de le sauvegarder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNBbFY985xeM"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hn-oC8HmWGNE"
      },
      "source": [
        "rl_model.save_weights('linear_regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jseO56VGaDeV"
      },
      "source": [
        "Initalisons à nouveau notre modèlre à random."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zgfnYkWZh4z"
      },
      "source": [
        "rl_model = linear_regression(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0I2c9GpfZpWc",
        "outputId": "a38fb8a6-470c-45df-fa3d-b9da0c6647ce"
      },
      "source": [
        "prediction = rl_model(X_train_tf)\n",
        "mean_absolute_error(prediction, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22.97079646114197"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C92sBoBvaJuW"
      },
      "source": [
        "Nous pouvons revenir à nos poids entraîner en charger nos précédens poids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64FFwodfZZGc",
        "outputId": "c588d5d1-e485-4a95-dfd8-e6586823f319"
      },
      "source": [
        "rl_model.load_weights('linear_regression')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1ce2b93ad0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7E-voudZqoy",
        "outputId": "6c5f39a4-5609-464e-aa97-ae3acceb44cc"
      },
      "source": [
        "prediction = rl_model(X_train_tf)\n",
        "mean_absolute_error(prediction, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13.128006919928355"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ie0R30phFWv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}